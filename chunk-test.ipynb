{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennsieve import Pennsieve\n",
    "from pennsieve.log import get_logger\n",
    "from pennsieve.api.agent import agent_cmd\n",
    "from pennsieve.api.agent import AgentError, check_port, socket_address\n",
    "import time\n",
    "import os\n",
    "from os.path import (\n",
    "    isdir,\n",
    "    isfile,\n",
    "    join,\n",
    "    splitext,\n",
    "    getmtime,\n",
    "    basename,\n",
    "    normpath,\n",
    "    exists,\n",
    "    expanduser,\n",
    "    split,\n",
    "    dirname,\n",
    "    getsize,\n",
    "    abspath,\n",
    ")\n",
    "import subprocess\n",
    "import socket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a soda JSON object that represents a BF upload\n",
    "sodaJSONObject = {\n",
    "    \"bf-account-selected\": {\n",
    "        \"account-name\": \"SODA-Pennsieve\"\n",
    "    },\n",
    "    \"bf-dataset-selected\": {\n",
    "        \"dataset-name\": \"chex\"\n",
    "    },\n",
    "    \"dataset-structure\": {\n",
    "        \"folders\": {\n",
    "            \"code\": {\n",
    "                \"type\": \"bf\",\n",
    "                \"action\": [\n",
    "                    \"existing\"\n",
    "                ],\n",
    "                \"path\": \"N:collection:06cad972-3ee3-40eb-bc6b-031d548aadea\",\n",
    "                \"folders\": {},\n",
    "                \"files\": {\n",
    "                    \"submission.xlsx\": {\n",
    "                        \"type\": \"bf\",\n",
    "                        \"action\": [\n",
    "                            \"existing\"\n",
    "                        ],\n",
    "                        \"path\": \"N:package:bb959750-17af-4961-80ad-d57c58487bd6\",\n",
    "                        \"timestamp\": \"2021-12-28T02:09:44,05658Z\",\n",
    "                        \"bfpath\": [\n",
    "                            \"code\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"submission (1).xlsx\": {\n",
    "                        \"type\": \"bf\",\n",
    "                        \"action\": [\n",
    "                            \"existing\"\n",
    "                        ],\n",
    "                        \"path\": \"N:package:dc819664-e498-4a5f-b40d-93ccc0e3aea2\",\n",
    "                        \"timestamp\": \"2021-12-29T21:51:15,659823Z\",\n",
    "                        \"bfpath\": [\n",
    "                            \"code\"\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"bfpath\": [\n",
    "                    \"code\"\n",
    "                ]\n",
    "            },\n",
    "            \"primary\": {\n",
    "                \"folders\": {\n",
    "                    \"sub-1-primary\": {\n",
    "                        \"folders\": {},\n",
    "                        \"files\": {\n",
    "                            \"summary.csv\": {\n",
    "                                \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\summary.csv\",\n",
    "                                \"type\": \"local\",\n",
    "                                \"description\": \"\",\n",
    "                                \"additional-metadata\": \"\",\n",
    "                                \"action\": [\n",
    "                                    \"new\"\n",
    "                                ]\n",
    "                            },\n",
    "                            \"term.csv\": {\n",
    "                                \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\term.csv\",\n",
    "                                \"type\": \"local\",\n",
    "                                \"description\": \"\",\n",
    "                                \"additional-metadata\": \"\",\n",
    "                                \"action\": [\n",
    "                                    \"new\"\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        \"type\": \"virtual\",\n",
    "                        \"action\": [\n",
    "                            \"new\",\n",
    "                            \"renamed\"\n",
    "                        ],\n",
    "                        \"basename\": \"sub-1-primary\"\n",
    "                    }\n",
    "                },\n",
    "                \"files\": {\n",
    "                    \"summary.csv\": {\n",
    "                        \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\summary.csv\",\n",
    "                        \"type\": \"local\",\n",
    "                        \"description\": \"\",\n",
    "                        \"additional-metadata\": \"\",\n",
    "                        \"action\": [\n",
    "                            \"new\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"term.csv\": {\n",
    "                        \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\term.csv\",\n",
    "                        \"type\": \"local\",\n",
    "                        \"description\": \"\",\n",
    "                        \"additional-metadata\": \"\",\n",
    "                        \"action\": [\n",
    "                            \"new\"\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"type\": \"\",\n",
    "                \"action\": {}\n",
    "            },\n",
    "            \"source\": {\n",
    "                \"folders\": {\n",
    "                    \"sub-1-source\": {\n",
    "                        \"folders\": {},\n",
    "                        \"files\": {\n",
    "                            \"file.csv\": {\n",
    "                                \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\file.csv\",\n",
    "                                \"type\": \"local\",\n",
    "                                \"description\": \"\",\n",
    "                                \"additional-metadata\": \"\",\n",
    "                                \"action\": [\n",
    "                                    \"new\"\n",
    "                                ]\n",
    "                            },\n",
    "                            \"human_subject.csv\": {\n",
    "                                \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\human_subject.csv\",\n",
    "                                \"type\": \"local\",\n",
    "                                \"description\": \"\",\n",
    "                                \"additional-metadata\": \"\",\n",
    "                                \"action\": [\n",
    "                                    \"new\"\n",
    "                                ]\n",
    "                            },\n",
    "                            \"protocol.csv\": {\n",
    "                                \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\protocol.csv\",\n",
    "                                \"type\": \"local\",\n",
    "                                \"description\": \"\",\n",
    "                                \"additional-metadata\": \"\",\n",
    "                                \"action\": [\n",
    "                                    \"new\"\n",
    "                                ]\n",
    "                            },\n",
    "                            \"researcher.csv\": {\n",
    "                                \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\researcher.csv\",\n",
    "                                \"type\": \"local\",\n",
    "                                \"description\": \"\",\n",
    "                                \"additional-metadata\": \"\",\n",
    "                                \"action\": [\n",
    "                                    \"new\"\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        \"type\": \"virtual\",\n",
    "                        \"action\": [\n",
    "                            \"new\",\n",
    "                            \"renamed\"\n",
    "                        ],\n",
    "                        \"basename\": \"sub-1-source\"\n",
    "                    }\n",
    "                },\n",
    "                \"files\": {\n",
    "                    \"protocol.csv\": {\n",
    "                        \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\protocol.csv\",\n",
    "                        \"type\": \"local\",\n",
    "                        \"description\": \"\",\n",
    "                        \"additional-metadata\": \"\",\n",
    "                        \"action\": [\n",
    "                            \"new\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"award.csv\": {\n",
    "                        \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\award.csv\",\n",
    "                        \"type\": \"local\",\n",
    "                        \"description\": \"\",\n",
    "                        \"additional-metadata\": \"\",\n",
    "                        \"action\": [\n",
    "                            \"new\"\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"type\": \"\",\n",
    "                \"action\": {}\n",
    "            },\n",
    "            \"docs\": {\n",
    "                \"type\": \"bf\",\n",
    "                \"action\": [\n",
    "                    \"existing\"\n",
    "                ],\n",
    "                \"path\": \"N:collection:2e4a062e-5dc8-438b-bac1-cb9108b5e828\",\n",
    "                \"folders\": {\n",
    "                    \"sub-1-docs\": {\n",
    "                        \"type\": \"bf\",\n",
    "                        \"action\": [\n",
    "                            \"existing\"\n",
    "                        ],\n",
    "                        \"path\": \"N:collection:62e3829b-cd1a-48a8-9791-db1db5f4240c\",\n",
    "                        \"folders\": {},\n",
    "                        \"files\": {},\n",
    "                        \"bfpath\": [\n",
    "                            \"docs\",\n",
    "                            \"sub-1-docs\"\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"files\": {},\n",
    "                \"bfpath\": [\n",
    "                    \"docs\"\n",
    "                ]\n",
    "            },\n",
    "            \"protocol\": {\n",
    "                \"folders\": {},\n",
    "                \"files\": {\n",
    "                    \"samples.csv\": {\n",
    "                        \"path\":\"C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\samples.csv\",\n",
    "                        \"type\": \"local\",\n",
    "                        \"description\": \"\",\n",
    "                        \"additional-metadata\": \"\",\n",
    "                        \"action\": [\n",
    "                            \"new\"\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"type\": \"\",\n",
    "                \"action\": {}\n",
    "            },\n",
    "            \"derivative\": {\n",
    "                \"type\": \"bf\",\n",
    "                \"action\": [\n",
    "                    \"existing\"\n",
    "                ],\n",
    "                \"path\": \"N:collection:5a5e8df9-6170-4796-aa2f-a0ee91eb32df\",\n",
    "                \"folders\": {},\n",
    "                \"files\": {\n",
    "                    \"Variance_AcrossIndividuals_BetweenSexes_Activation.xlsx\": {\n",
    "                        \"type\": \"bf\",\n",
    "                        \"action\": [\n",
    "                            \"existing\"\n",
    "                        ],\n",
    "                        \"path\": \"N:package:bae5639e-33ea-4a05-bcb2-c7120f57f4d2\",\n",
    "                        \"timestamp\": \"2022-01-12T18:18:36,210109Z\",\n",
    "                        \"bfpath\": [\n",
    "                            \"derivative\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"Variance_AcrossIndividuals_BetweenSexes_Block.xlsx\": {\n",
    "                        \"type\": \"bf\",\n",
    "                        \"action\": [\n",
    "                            \"existing\"\n",
    "                        ],\n",
    "                        \"path\": \"N:package:11284cdc-b29f-4b00-9f3b-d5b2c5ba3667\",\n",
    "                        \"timestamp\": \"2022-01-12T18:18:37,37515Z\",\n",
    "                        \"bfpath\": [\n",
    "                            \"derivative\"\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"bfpath\": [\n",
    "                    \"derivative\"\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"files\": {}\n",
    "    },\n",
    "    \"metadata-files\": {\n",
    "        \"submission.xlsx\": {\n",
    "            \"type\": \"bf\",\n",
    "            \"action\": [\n",
    "                \"existing\"\n",
    "            ],\n",
    "            \"path\": \"N:package:6a5343d5-3da9-4d49-baca-a62d0fe625c1\"\n",
    "        },\n",
    "        \"CHANGES.txt\": {\n",
    "            \"type\": \"bf\",\n",
    "            \"action\": [\n",
    "                \"existing\"\n",
    "            ],\n",
    "            \"path\": \"N:package:f0c514c3-bd73-4e4c-8814-0b97b3f69556\"\n",
    "        },\n",
    "        \"README.txt\": {\n",
    "            \"type\": \"bf\",\n",
    "            \"action\": [\n",
    "                \"existing\"\n",
    "            ],\n",
    "            \"path\": \"N:package:bd96b506-e79d-4b45-b342-35c881fbd5fc\"\n",
    "        }\n",
    "    },\n",
    "    \"generate-dataset\": {\n",
    "        \"destination\": \"bf\",\n",
    "        \"generate-option\": \"existing-bf\"\n",
    "    },\n",
    "    \"starting-point\": {\n",
    "        \"type\": \"bf\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_recognized_file_extensions = [\n",
    "    \".cram\",\n",
    "    \".jp2\",\n",
    "    \".jpx\",\n",
    "    \".lsm\",\n",
    "    \".ndpi\",\n",
    "    \".nifti\",\n",
    "    \".oib\",\n",
    "    \".oif\",\n",
    "    \".roi\",\n",
    "    \".rtf\",\n",
    "    \".swc\",\n",
    "    \".abf\",\n",
    "    \".acq\",\n",
    "    \".adicht\",\n",
    "    \".adidat\",\n",
    "    \".aedt\",\n",
    "    \".afni\",\n",
    "    \".ai\",\n",
    "    \".avi\",\n",
    "    \".bam\",\n",
    "    \".bash\",\n",
    "    \".bcl\",\n",
    "    \".bcl.gz\",\n",
    "    \".bin\",\n",
    "    \".brik\",\n",
    "    \".brukertiff.gz\",\n",
    "    \".continuous\",\n",
    "    \".cpp\",\n",
    "    \".csv\",\n",
    "    \".curv\",\n",
    "    \".cxls\",\n",
    "    \".czi\",\n",
    "    \".data\",\n",
    "    \".dcm\",\n",
    "    \".df\",\n",
    "    \".dicom\",\n",
    "    \".doc\",\n",
    "    \".docx\",\n",
    "    \".e\",\n",
    "    \".edf\",\n",
    "    \".eps\",\n",
    "    \".events\",\n",
    "    \".fasta\",\n",
    "    \".fastq\",\n",
    "    \".fcs\",\n",
    "    \".feather\",\n",
    "    \".fig\",\n",
    "    \".gif\",\n",
    "    \".h4\",\n",
    "    \".h5\",\n",
    "    \".hdf4\",\n",
    "    \".hdf5\",\n",
    "    \".hdr\",\n",
    "    \".he2\",\n",
    "    \".he5\",\n",
    "    \".head\",\n",
    "    \".hoc\",\n",
    "    \".htm\",\n",
    "    \".html\",\n",
    "    \".ibw\",\n",
    "    \".img\",\n",
    "    \".ims\",\n",
    "    \".ipynb\",\n",
    "    \".jpeg\",\n",
    "    \".jpg\",\n",
    "    \".js\",\n",
    "    \".json\",\n",
    "    \".lay\",\n",
    "    \".lh\",\n",
    "    \".lif\",\n",
    "    \".m\",\n",
    "    \".mat\",\n",
    "    \".md\",\n",
    "    \".mef\",\n",
    "    \".mefd.gz\",\n",
    "    \".mex\",\n",
    "    \".mgf\",\n",
    "    \".mgh\",\n",
    "    \".mgh.gz\",\n",
    "    \".mgz\",\n",
    "    \".mnc\",\n",
    "    \".moberg.gz\",\n",
    "    \".mod\",\n",
    "    \".mov\",\n",
    "    \".mp4\",\n",
    "    \".mph\",\n",
    "    \".mpj\",\n",
    "    \".mtw\",\n",
    "    \".ncs\",\n",
    "    \".nd2\",\n",
    "    \".nev\",\n",
    "    \".nex\",\n",
    "    \".nex5\",\n",
    "    \".nf3\",\n",
    "    \".nii\",\n",
    "    \".nii.gz\",\n",
    "    \".ns1\",\n",
    "    \".ns2\",\n",
    "    \".ns3\",\n",
    "    \".ns4\",\n",
    "    \".ns5\",\n",
    "    \".ns6\",\n",
    "    \".nwb\",\n",
    "    \".ogg\",\n",
    "    \".ogv\",\n",
    "    \".ome.btf\",\n",
    "    \".ome.tif\",\n",
    "    \".ome.tif2\",\n",
    "    \".ome.tif8\",\n",
    "    \".ome.tiff\",\n",
    "    \".ome.xml\",\n",
    "    \".openephys\",\n",
    "    \".pdf\",\n",
    "    \".pgf\",\n",
    "    \".png\",\n",
    "    \".ppt\",\n",
    "    \".pptx\",\n",
    "    \".ps\",\n",
    "    \".pul\",\n",
    "    \".py\",\n",
    "    \".r\",\n",
    "    \".raw\",\n",
    "    \".rdata\",\n",
    "    \".rh\",\n",
    "    \".rhd\",\n",
    "    \".sh\",\n",
    "    \".sldasm\",\n",
    "    \".slddrw\",\n",
    "    \".smr\",\n",
    "    \".spikes\",\n",
    "    \".svg\",\n",
    "    \".svs\",\n",
    "    \".tab\",\n",
    "    \".tar\",\n",
    "    \".tar.gz\",\n",
    "    \".tcsh\",\n",
    "    \".tdm\",\n",
    "    \".tdms\",\n",
    "    \".text\",\n",
    "    \".tif\",\n",
    "    \".tiff\",\n",
    "    \".tsv\",\n",
    "    \".txt\",\n",
    "    \".vcf\",\n",
    "    \".webm\",\n",
    "    \".xlsx\",\n",
    "    \".xml\",\n",
    "    \".yaml\",\n",
    "    \".yml\",\n",
    "    \".zip\",\n",
    "    \".zsh\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_file_name(file_name):\n",
    "    output = []\n",
    "    if file_name[-1] == \")\":\n",
    "        string_length = len(file_name)\n",
    "        count_start = string_length\n",
    "        character = file_name[count_start - 1]\n",
    "        while character != \"(\" and count_start >= 0:\n",
    "            count_start -= 1\n",
    "            character = file_name[count_start - 1]\n",
    "        if character == \"(\":\n",
    "            base_name = file_name[0 : count_start - 1]\n",
    "            num = file_name[count_start : string_length - 1]\n",
    "            if check_if_int(num):\n",
    "                output = [base_name, int(num)]\n",
    "            return output\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "userpath = expanduser(\"~\")\n",
    "\n",
    "# path to local SODA folder for saving manifest files\n",
    "manifest_sparc = [\"manifest.xlsx\", \"manifest.csv\"]\n",
    "manifest_folder_path = join(userpath, \"SODA\", \"manifest_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_forbidden_characters_bf(my_string):\n",
    "    \"\"\"\n",
    "    Check for forbidden characters in Pennsieve file/folder name\n",
    "\n",
    "    Args:\n",
    "        my_string: string with characters (string)\n",
    "    Returns:\n",
    "        False: no forbidden character\n",
    "        True: presence of forbidden character(s)\n",
    "    \"\"\"\n",
    "    regex = re.compile(\"[\" + forbidden_characters_bf + \"]\")\n",
    "    if regex.search(my_string) == None and \"\\\\\" not in r\"%r\" % my_string:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_dataset_size():\n",
    "    \"\"\"\n",
    "    Function to get storage size of a dataset on Pennsieve\n",
    "    \"\"\"\n",
    "    global bf\n",
    "    global myds\n",
    "\n",
    "    try:\n",
    "        selected_dataset_id = myds.id\n",
    "        bf_response = bf._api._get(\"/datasets/\" + str(selected_dataset_id))\n",
    "        return bf_response[\"storage\"] if \"storage\" in bf_response.keys() else 0\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_running():\n",
    "    listen_port = 11235\n",
    "\n",
    "    try:\n",
    "        # x = \"ws://127.0.0.1:11235\"\n",
    "        # create_connection(x).close()\n",
    "        # CHANGE BACK\n",
    "        create_connection(socket_address(listen_port)).close()\n",
    "\n",
    "    except socket.error as e:\n",
    "\n",
    "        if e.errno == errno.ECONNREFUSED:  # ConnectionRefusedError for Python 3\n",
    "            return True\n",
    "        else:\n",
    "            raise e\n",
    "    else:\n",
    "        raise AgentError(\n",
    "            \"The Pennsieve agent is already running. Learn more about how to solve the issue <a href='https://github.com/bvhpatel/SODA/wiki/The-Pennsieve-agent-is-already-running' target='_blank'>here</a>.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_queue():\n",
    "\n",
    "    command = [agent_cmd(), \"upload-status\", \"--cancel-all\"]\n",
    "\n",
    "    proc = subprocess.run(command, check=True)  # env=agent_env(?settings?)\n",
    "    return proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relative_path(x, y):\n",
    "    if x:\n",
    "        relative_path = x + \"/\" + y\n",
    "    else:\n",
    "        relative_path = y\n",
    "    return relative_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_get_existing_files_details(bf_folder):\n",
    "\n",
    "    double_extensions = [\n",
    "        \".ome.tiff\",\n",
    "        \".ome.tif\",\n",
    "        \".ome.tf2,\",\n",
    "        \".ome.tf8\",\n",
    "        \".ome.btf\",\n",
    "        \".ome.xml\",\n",
    "        \".brukertiff.gz\",\n",
    "        \".mefd.gz\",\n",
    "        \".moberg.gz\",\n",
    "        \".nii.gz\",\n",
    "        \".mgh.gz\",\n",
    "        \".tar.gz\",\n",
    "        \".bcl.gz\",\n",
    "    ]\n",
    "\n",
    "    # f = open(\"dataset_contents.soda\", \"a\")\n",
    "\n",
    "    def verify_file_name(file_name, extension):\n",
    "        if extension == \"\":\n",
    "            return file_name\n",
    "\n",
    "        double_ext = False\n",
    "        for ext in double_extensions:\n",
    "            if file_name.find(ext) != -1:\n",
    "                double_ext = True\n",
    "                break\n",
    "\n",
    "        extension_from_name = \"\"\n",
    "\n",
    "        if double_ext == False:\n",
    "            extension_from_name = os.path.splitext(file_name)[1]\n",
    "        else:\n",
    "            extension_from_name = (\n",
    "                os.path.splitext(os.path.splitext(file_name)[0])[1]\n",
    "                + os.path.splitext(file_name)[1]\n",
    "            )\n",
    "\n",
    "        if extension_from_name == (\".\" + extension):\n",
    "            return file_name\n",
    "        else:\n",
    "            return file_name + (\".\" + extension)\n",
    "\n",
    "    bf_existing_files = [x for x in bf_folder.items if x.type != \"Collection\"]\n",
    "    bf_existing_files_name = [splitext(x.name)[0] for x in bf_existing_files]\n",
    "    bf_existing_files_name_with_extension = []\n",
    "    for file in bf_existing_files:\n",
    "        file_name_with_extension = \"\"\n",
    "        file_id = file.id\n",
    "        file_details = bf._api._get(\"/packages/\" + str(file_id))\n",
    "        # file_name_with_extension = verify_file_name(file_details[\"content\"][\"name\"], file_details[\"extension\"])\n",
    "        if \"extension\" not in file_details:\n",
    "            file_name_with_extension = verify_file_name(\n",
    "                file_details[\"content\"][\"name\"], \"\"\n",
    "            )\n",
    "        else:\n",
    "            file_name_with_extension = verify_file_name(\n",
    "                file_details[\"content\"][\"name\"], file_details[\"extension\"]\n",
    "            )\n",
    "\n",
    "        # file_extension = splitext(file_name_with_extension)[1]\n",
    "        # file_name_with_extension = splitext(file.name)[0] + file_extension\n",
    "        bf_existing_files_name_with_extension.append(file_name_with_extension)\n",
    "\n",
    "    return (\n",
    "        bf_existing_files,\n",
    "        bf_existing_files_name,\n",
    "        bf_existing_files_name_with_extension,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_get_existing_folders_details(bf_folder):\n",
    "    bf_existing_folders = [x for x in bf_folder.items if x.type == \"Collection\"]\n",
    "    bf_existing_folders_name = [x.name for x in bf_existing_folders]\n",
    "\n",
    "    return bf_existing_folders, bf_existing_folders_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_get_dataset_files_folders(soda_json_structure, requested_sparc_only=True):\n",
    "    \"\"\"\n",
    "    Function for importing Pennsieve data files info into the \"dataset-structure\" key of the soda json structure,\n",
    "    including metadata from any existing manifest files in the high-level folders\n",
    "    (name, id, timestamp, description, additional metadata)\n",
    "\n",
    "    Args:\n",
    "        soda_json_structure: soda structure with bf account and dataset info available\n",
    "    Output:\n",
    "        same soda structure with Pennsieve data file info included under the \"dataset-structure\" key\n",
    "    \"\"\"\n",
    "\n",
    "    high_level_sparc_folders = [\n",
    "        \"code\",\n",
    "        \"derivative\",\n",
    "        \"docs\",\n",
    "        \"primary\",\n",
    "        \"protocol\",\n",
    "        \"source\",\n",
    "    ]\n",
    "    manifest_sparc = [\"manifest.xlsx\", \"manifest.csv\"]\n",
    "    high_level_metadata_sparc = [\n",
    "        \"submission.xlsx\",\n",
    "        \"submission.csv\",\n",
    "        \"submission.json\",\n",
    "        \"dataset_description.xlsx\",\n",
    "        \"dataset_description.csv\",\n",
    "        \"dataset_description.json\",\n",
    "        \"subjects.xlsx\",\n",
    "        \"subjects.csv\",\n",
    "        \"subjects.json\",\n",
    "        \"samples.xlsx\",\n",
    "        \"samples.csv\",\n",
    "        \"samples.json\",\n",
    "        \"README.txt\",\n",
    "        \"CHANGES.txt\",\n",
    "        \"code_description.xlsx\",\n",
    "        \"inputs_metadata.xlsx\",\n",
    "        \"outputs_metadata.xlsx\",\n",
    "    ]\n",
    "    manifest_error_message = []\n",
    "    double_extensions = [\n",
    "        \".ome.tiff\",\n",
    "        \".ome.tif\",\n",
    "        \".ome.tf2,\",\n",
    "        \".ome.tf8\",\n",
    "        \".ome.btf\",\n",
    "        \".ome.xml\",\n",
    "        \".brukertiff.gz\",\n",
    "        \".mefd.gz\",\n",
    "        \".moberg.gz\",\n",
    "        \".nii.gz\",\n",
    "        \".mgh.gz\",\n",
    "        \".tar.gz\",\n",
    "        \".bcl.gz\",\n",
    "    ]\n",
    "\n",
    "    # f = open(\"dataset_contents.soda\", \"a\")\n",
    "\n",
    "    def verify_file_name(file_name, extension):\n",
    "        if extension == \"\":\n",
    "            return file_name\n",
    "\n",
    "        double_ext = False\n",
    "        for ext in double_extensions:\n",
    "            if file_name.find(ext) != -1:\n",
    "                double_ext = True\n",
    "                break\n",
    "\n",
    "        extension_from_name = \"\"\n",
    "\n",
    "        if double_ext == False:\n",
    "            extension_from_name = os.path.splitext(file_name)[1]\n",
    "        else:\n",
    "            extension_from_name = (\n",
    "                os.path.splitext(os.path.splitext(file_name)[0])[1]\n",
    "                + os.path.splitext(file_name)[1]\n",
    "            )\n",
    "\n",
    "        if extension_from_name == (\".\" + extension):\n",
    "            return file_name\n",
    "        else:\n",
    "            return file_name + (\".\" + extension)\n",
    "\n",
    "    # Add a new key containing the path to all the files and folders on the\n",
    "    # local data structure..\n",
    "    def recursive_item_path_create(folder, path):\n",
    "        if \"files\" in folder.keys():\n",
    "            for item in list(folder[\"files\"]):\n",
    "                if \"bfpath\" not in folder[\"files\"][item]:\n",
    "                    folder[\"files\"][item][\"bfpath\"] = path[:]\n",
    "\n",
    "        if \"folders\" in folder.keys():\n",
    "            for item in list(folder[\"folders\"]):\n",
    "                if \"bfpath\" not in folder[\"folders\"][item]:\n",
    "                    folder[\"folders\"][item][\"bfpath\"] = path[:]\n",
    "                    folder[\"folders\"][item][\"bfpath\"].append(item)\n",
    "                recursive_item_path_create(\n",
    "                    folder[\"folders\"][item], folder[\"folders\"][item][\"bfpath\"][:]\n",
    "                )\n",
    "        return\n",
    "\n",
    "    level = 0\n",
    "\n",
    "    def recursive_dataset_import(\n",
    "        my_item, dataset_folder, metadata_files, my_folder_name, my_level, manifest_dict\n",
    "    ):\n",
    "        level = 0\n",
    "        col_count = 0\n",
    "        file_count = 0\n",
    "\n",
    "        for item in my_item:\n",
    "            if item.type == \"Collection\":\n",
    "                if \"folders\" not in dataset_folder:\n",
    "                    dataset_folder[\"folders\"] = {}\n",
    "                if \"files\" not in dataset_folder:\n",
    "                    dataset_folder[\"files\"] = {}\n",
    "                col_count += 1\n",
    "                folder_name = item.name\n",
    "                if (\n",
    "                    my_level == 0\n",
    "                    and folder_name not in high_level_sparc_folders\n",
    "                    and requested_sparc_only\n",
    "                ):  # only import SPARC folders\n",
    "                    col_count -= 1\n",
    "                    continue\n",
    "                if col_count == 1:\n",
    "                    level = my_level + 1\n",
    "                dataset_folder[\"folders\"][folder_name] = {\n",
    "                    \"type\": \"bf\",\n",
    "                    \"action\": [\"existing\"],\n",
    "                    \"path\": item.id,\n",
    "                }\n",
    "                sub_folder = dataset_folder[\"folders\"][folder_name]\n",
    "                if \"folders\" not in sub_folder:\n",
    "                    sub_folder[\"folders\"] = {}\n",
    "                if \"files\" not in sub_folder:\n",
    "                    sub_folder[\"files\"] = {}\n",
    "                recursive_dataset_import(\n",
    "                    item, sub_folder, metadata_files, folder_name, level, manifest_dict\n",
    "                )\n",
    "            else:\n",
    "                if \"folders\" not in dataset_folder:\n",
    "                    dataset_folder[\"folders\"] = {}\n",
    "                if \"files\" not in dataset_folder:\n",
    "                    dataset_folder[\"files\"] = {}\n",
    "                package_id = item.id\n",
    "                package_details = bf._api._get(\"/packages/\" + str(package_id))\n",
    "                if \"extension\" not in package_details:\n",
    "                    file_name = verify_file_name(package_details[\"content\"][\"name\"], \"\")\n",
    "                else:\n",
    "                    file_name = verify_file_name(\n",
    "                        package_details[\"content\"][\"name\"], package_details[\"extension\"]\n",
    "                    )\n",
    "\n",
    "                if my_level == 0 and file_name in high_level_metadata_sparc:\n",
    "                    metadata_files[file_name] = {\n",
    "                        \"type\": \"bf\",\n",
    "                        \"action\": [\"existing\"],\n",
    "                        \"path\": item.id,\n",
    "                    }\n",
    "\n",
    "                else:\n",
    "                    file_count += 1\n",
    "                    if my_level == 1 and file_name in manifest_sparc:\n",
    "                        file_details = bf._api._get(\n",
    "                            \"/packages/\" + str(package_id) + \"/view\"\n",
    "                        )\n",
    "                        file_id = file_details[0][\"content\"][\"id\"]\n",
    "                        manifest_url = bf._api._get(\n",
    "                            \"/packages/\" + str(package_id) + \"/files/\" + str(file_id)\n",
    "                        )\n",
    "                        df = \"\"\n",
    "                        try:\n",
    "                            if file_name.lower() == \"manifest.xlsx\":\n",
    "                                df = pd.read_excel(\n",
    "                                    manifest_url[\"url\"], engine=\"openpyxl\"\n",
    "                                )\n",
    "                            else:\n",
    "                                df = pd.read_csv(manifest_url[\"url\"])\n",
    "                            manifest_dict[my_folder_name] = df\n",
    "                        except Exception as e:\n",
    "                            manifest_error_message.append(\n",
    "                                package_details[\"parent\"][\"content\"][\"name\"]\n",
    "                            )\n",
    "                            pass\n",
    "                    else:\n",
    "                        timestamp = (\n",
    "                            package_details[\"content\"][\"createdAt\"]\n",
    "                            .replace(\".\", \",\")\n",
    "                            .replace(\"+00:00\", \"Z\")\n",
    "                        )\n",
    "                        dataset_folder[\"files\"][file_name] = {\n",
    "                            \"type\": \"bf\",\n",
    "                            \"action\": [\"existing\"],\n",
    "                            \"path\": item.id,\n",
    "                            \"timestamp\": timestamp,\n",
    "                        }\n",
    "\n",
    "    def recursive_manifest_info_import(my_folder, my_relative_path, manifest_df):\n",
    "\n",
    "        if \"files\" in my_folder.keys():\n",
    "            for file_key, file in my_folder[\"files\"].items():\n",
    "                filename = join(my_relative_path, file_key)\n",
    "                colum_headers = manifest_df.columns.tolist()\n",
    "                filename = filename.replace(\"\\\\\", \"/\")\n",
    "\n",
    "                if filename in list(manifest_df[\"filename\"].values):\n",
    "                    if \"description\" in colum_headers:\n",
    "                        mydescription = manifest_df[\n",
    "                            manifest_df[\"filename\"] == filename\n",
    "                        ][\"description\"].values[0]\n",
    "                        if mydescription:\n",
    "                            file[\"description\"] = mydescription\n",
    "                    if \"Additional Metadata\" in colum_headers:\n",
    "                        my_additional_medata = manifest_df[\n",
    "                            manifest_df[\"filename\"] == filename\n",
    "                        ][\"Additional Metadata\"].values[0]\n",
    "                        if my_additional_medata:\n",
    "                            file[\"additional-metadata\"] = my_additional_medata\n",
    "                    if \"timestamp\" in colum_headers:\n",
    "                        my_timestamp = manifest_df[manifest_df[\"filename\"] == filename][\n",
    "                            \"timestamp\"\n",
    "                        ].values[0]\n",
    "                        if my_timestamp:\n",
    "                            file[\"timestamp\"] = my_timestamp\n",
    "\n",
    "        if \"folders\" in my_folder.keys():\n",
    "            for folder_key, folder in my_folder[\"folders\"].items():\n",
    "                relative_path = join(my_relative_path, folder_key)\n",
    "\n",
    "                recursive_manifest_info_import(folder, relative_path, manifest_df)\n",
    "\n",
    "    # START\n",
    "\n",
    "    error = []\n",
    "\n",
    "    # check that the Pennsieve account is valid\n",
    "    try:\n",
    "        bf_account_name = soda_json_structure[\"bf-account-selected\"][\"account-name\"]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        bf = Pennsieve(bf_account_name)\n",
    "    except Exception as e:\n",
    "        error.append(\"Error: Please select a valid Pennsieve account\")\n",
    "        raise Exception(error)\n",
    "\n",
    "    # check that the Pennsieve dataset is valid\n",
    "    try:\n",
    "        bf_dataset_name = soda_json_structure[\"bf-dataset-selected\"][\"dataset-name\"]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    try:\n",
    "        myds = bf.get_dataset(bf_dataset_name)\n",
    "    except Exception as e:\n",
    "        error.append(\"Error: Please select a valid Pennsieve dataset\")\n",
    "        raise Exception(error)\n",
    "\n",
    "    # check that the user has permission to edit this dataset\n",
    "    try:\n",
    "        role = bf_get_current_user_permission(bf, myds)\n",
    "        if role not in [\"owner\", \"manager\", \"editor\"]:\n",
    "            curatestatus = \"Done\"\n",
    "            error.append(\n",
    "                \"Error: You don't have permissions for uploading to this Pennsieve dataset\"\n",
    "            )\n",
    "            raise Exception(error)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        # import files and folders in the soda json structure\n",
    "        soda_json_structure[\"dataset-structure\"] = {}\n",
    "        soda_json_structure[\"metadata-files\"] = {}\n",
    "        dataset_folder = soda_json_structure[\"dataset-structure\"]\n",
    "        metadata_files = soda_json_structure[\"metadata-files\"]\n",
    "        manifest_dict = {}\n",
    "        folder_name = \"\"\n",
    "        recursive_dataset_import(\n",
    "            myds, dataset_folder, metadata_files, folder_name, level, manifest_dict\n",
    "        )\n",
    "\n",
    "        # remove metadata files keys if empty\n",
    "        metadata_files = soda_json_structure[\"metadata-files\"]\n",
    "        if not metadata_files:\n",
    "            del soda_json_structure[\"metadata-files\"]\n",
    "\n",
    "        dataset_folder = soda_json_structure[\"dataset-structure\"]\n",
    "        # pull information from the manifest files if they satisfy the SPARC format\n",
    "        if \"folders\" in dataset_folder.keys():\n",
    "            for folder_key in manifest_dict.keys():\n",
    "                manifest_df = manifest_dict[folder_key]\n",
    "                manifest_df = manifest_df.fillna(\"\")\n",
    "                colum_headers = manifest_df.columns.tolist()\n",
    "                folder = dataset_folder[\"folders\"][folder_key]\n",
    "                if \"filename\" in colum_headers:\n",
    "                    if (\n",
    "                        \"description\" in colum_headers\n",
    "                        or \"Additional Metadata\" in colum_headers\n",
    "                    ):\n",
    "                        relative_path = \"\"\n",
    "                        recursive_manifest_info_import(\n",
    "                            folder, relative_path, manifest_df\n",
    "                        )\n",
    "\n",
    "        recursive_item_path_create(soda_json_structure[\"dataset-structure\"], [])\n",
    "        success_message = (\n",
    "            \"Data files under a valid high-level SPARC folders have been imported\"\n",
    "        )\n",
    "        return [soda_json_structure, success_message, manifest_error_message]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_get_current_user_permission(bf, myds):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to get the permission of currently logged in user for a selected dataset\n",
    "\n",
    "    Args:\n",
    "        bf: logged Pennsieve acccount (dict)\n",
    "        myds: selected Pennsieve dataset (dict)\n",
    "    Output:\n",
    "        permission of current user (string)\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        selected_dataset_id = myds.id\n",
    "        user_role = bf._api._get(\"/datasets/\" + str(selected_dataset_id) + \"/role\")[\n",
    "            \"role\"\n",
    "        ]\n",
    "\n",
    "        return user_role\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_local_dataset_files_validity(soda_json_structure):\n",
    "    \"\"\"\n",
    "    Function to check that the local data files and folders specified in the dataset are valid\n",
    "\n",
    "    Args:\n",
    "        soda_json_structure: soda dict with information about all specified files and folders\n",
    "    Output:\n",
    "        error: error message with list of non valid local data files, if any\n",
    "    \"\"\"\n",
    "\n",
    "    def recursive_local_file_check(my_folder, my_relative_path, error):\n",
    "        for folder_key, folder in my_folder[\"folders\"].items():\n",
    "            relative_path = my_relative_path + \"/\" + folder_key\n",
    "            error = recursive_local_file_check(folder, relative_path, error)\n",
    "\n",
    "        for file_key in list(my_folder[\"files\"].keys()):\n",
    "            file = my_folder[\"files\"][file_key]\n",
    "            file_type = file[\"type\"]\n",
    "            if file_type == \"local\":\n",
    "                file_path = file[\"path\"]\n",
    "                if file[\"type\"] == \"bf\":\n",
    "                    continue\n",
    "                if not isfile(file_path):\n",
    "                    relative_path = my_relative_path + \"/\" + file_key\n",
    "                    error_message = relative_path + \" (path: \" + file_path + \")\"\n",
    "                    error.append(error_message)\n",
    "                else:\n",
    "                    file_size = getsize(file_path)\n",
    "                    if file_size == 0:\n",
    "                        del my_folder[\"files\"][file_key]\n",
    "\n",
    "        return error\n",
    "\n",
    "    def recursive_empty_local_folder_remove(\n",
    "        my_folder, my_folder_key, my_folders_content\n",
    "    ):\n",
    "\n",
    "        folders_content = my_folder[\"folders\"]\n",
    "        for folder_key in list(my_folder[\"folders\"].keys()):\n",
    "            folder = my_folder[\"folders\"][folder_key]\n",
    "            recursive_empty_local_folder_remove(folder, folder_key, folders_content)\n",
    "\n",
    "        if not my_folder[\"folders\"]:\n",
    "            if not my_folder[\"files\"]:\n",
    "                if my_folder[\"type\"] != \"bf\":\n",
    "                    del my_folders_content[my_folder_key]\n",
    "\n",
    "    error = []\n",
    "    if \"dataset-structure\" in soda_json_structure.keys():\n",
    "        dataset_structure = soda_json_structure[\"dataset-structure\"]\n",
    "        if \"folders\" in dataset_structure:\n",
    "            for folder_key, folder in dataset_structure[\"folders\"].items():\n",
    "                relative_path = folder_key\n",
    "                error = recursive_local_file_check(folder, relative_path, error)\n",
    "\n",
    "            folders_content = dataset_structure[\"folders\"]\n",
    "            for folder_key in list(dataset_structure[\"folders\"].keys()):\n",
    "                folder = dataset_structure[\"folders\"][folder_key]\n",
    "                recursive_empty_local_folder_remove(folder, folder_key, folders_content)\n",
    "\n",
    "    if \"metadata-files\" in soda_json_structure.keys():\n",
    "        metadata_files = soda_json_structure[\"metadata-files\"]\n",
    "        for file_key in list(metadata_files.keys()):\n",
    "            file = metadata_files[file_key]\n",
    "            file_type = file[\"type\"]\n",
    "            if file_type == \"local\":\n",
    "                file_path = file[\"path\"]\n",
    "                if not isfile(file_path):\n",
    "                    error_message = file_key + \" (path: \" + file_path + \")\"\n",
    "                    error.append(error_message)\n",
    "                else:\n",
    "                    file_size = getsize(file_path)\n",
    "                    if file_size == 0:\n",
    "                        del metadata_files[file_key]\n",
    "        if not metadata_files:\n",
    "            del soda_json_structure[\"metadata-files\"]\n",
    "\n",
    "    if len(error) > 0:\n",
    "        error_message = [\n",
    "            \"Error: The following local files were not found. Specify them again or remove them.\"\n",
    "        ]\n",
    "        error = error_message + error\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if a dataset that will be uploaded to Pennsieve has a valid structure\n",
    "def bf_check_dataset_files_validity(soda_json_structure, bf):\n",
    "    \"\"\"\n",
    "    Function to check that the bf data files and folders specified in the dataset are valid\n",
    "\n",
    "    Args:\n",
    "        dataset_structure: soda dict with information about all specified files and folders\n",
    "    Output:\n",
    "        error: error message with list of non valid local data files, if any\n",
    "    \"\"\"\n",
    "\n",
    "    def recursive_bf_dataset_check(my_folder, my_relative_path, error):\n",
    "        if \"folders\" in my_folder.keys():\n",
    "            for folder_key, folder in my_folder[\"folders\"].items():\n",
    "                folder_type = folder[\"type\"]\n",
    "                relative_path = my_relative_path + \"/\" + folder_key\n",
    "                if folder_type == \"bf\":\n",
    "                    package_id = folder[\"path\"]\n",
    "                    try:\n",
    "                        details = bf._api._get(\"/packages/\" + str(package_id) + \"/view\")\n",
    "                    except Exception as e:\n",
    "                        error_message = relative_path + \" (id: \" + package_id + \")\"\n",
    "                        error.append(error_message)\n",
    "                        pass\n",
    "                error = recursive_bf_dataset_check(folder, relative_path, error)\n",
    "        if \"files\" in my_folder.keys():\n",
    "            for file_key, file in my_folder[\"files\"].items():\n",
    "                file_type = file[\"type\"]\n",
    "                if file_type == \"bf\":\n",
    "                    package_id = file[\"path\"]\n",
    "                    try:\n",
    "                        details = bf._api._get(\"/packages/\" + str(package_id) + \"/view\")\n",
    "                    except Exception as e:\n",
    "                        relative_path = my_relative_path + \"/\" + file_key\n",
    "                        error_message = relative_path + \" (id: \" + package_id + \")\"\n",
    "                        error.append(error_message)\n",
    "                        pass\n",
    "\n",
    "        return error\n",
    "\n",
    "    error = []\n",
    "    if \"dataset-structure\" in soda_json_structure.keys():\n",
    "        dataset_structure = soda_json_structure[\"dataset-structure\"]\n",
    "        if \"folders\" in dataset_structure:\n",
    "            for folder_key, folder in dataset_structure[\"folders\"].items():\n",
    "                folder_type = folder[\"type\"]\n",
    "                relative_path = folder_key\n",
    "                if folder_type == \"bf\":\n",
    "                    package_id = folder[\"path\"]\n",
    "                    try:\n",
    "                        details = bf._api._get(\"/packages/\" + str(package_id) + \"/view\")\n",
    "                    except Exception as e:\n",
    "                        error_message = relative_path + \" (id: \" + package_id + \")\"\n",
    "                        error.append(error_message)\n",
    "                        pass\n",
    "                error = recursive_bf_dataset_check(folder, relative_path, error)\n",
    "        if \"files\" in dataset_structure:\n",
    "            for file_key, file in dataset_structure[\"files\"].items():\n",
    "                file_type = file[\"type\"]\n",
    "                if file_type == \"bf\":\n",
    "                    package_id = folder[\"path\"]\n",
    "                    try:\n",
    "                        details = bf._api._get(\"/packages/\" + str(package_id) + \"/view\")\n",
    "                    except Exception as e:\n",
    "                        relative_path = file_key\n",
    "                        error_message = relative_path + \" (id: \" + package_id + \")\"\n",
    "                        error.append(error_message)\n",
    "                        pass\n",
    "\n",
    "    if \"metadata-files\" in soda_json_structure:\n",
    "        metadata_files = soda_json_structure[\"metadata-files\"]\n",
    "        for file_key, file in metadata_files.items():\n",
    "            file_type = file[\"type\"]\n",
    "            if file_type == \"bf\":\n",
    "                package_id = file[\"path\"]\n",
    "                try:\n",
    "                    details = bf._api._get(\"/packages/\" + str(package_id) + \"/view\")\n",
    "                except Exception as e:\n",
    "                    error_message = file_key + \" (id: \" + package_id + \")\"\n",
    "                    error.append(error_message)\n",
    "                    pass\n",
    "\n",
    "    if len(error) > 0:\n",
    "        error_message = [\n",
    "            \"Error: The following Pennsieve files/folders are invalid. Specify them again or remove them.\"\n",
    "        ]\n",
    "        error = error_message + error\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main curate function works off of the SODA JSON object and uses the below to methods \n",
    "# to upload a dataset to Pennsieve\n",
    "def main_curate_function(soda_json_structure):\n",
    "\n",
    "    global main_curate_status\n",
    "    global main_curate_progress_message\n",
    "    global main_total_generate_dataset_size\n",
    "    global main_generated_dataset_size\n",
    "    global start_generate\n",
    "    global generate_start_time\n",
    "    global main_generate_destination\n",
    "    global main_initial_bfdataset_size\n",
    "    # global progress_percentage\n",
    "    # global progress_percentage_array\n",
    "\n",
    "    global bf\n",
    "    global myds\n",
    "\n",
    "    start_generate = 0\n",
    "    generate_start_time = time.time()\n",
    "\n",
    "    main_curate_status = \"\"\n",
    "    main_curate_progress_message = \"Starting...\"\n",
    "    main_total_generate_dataset_size = 1\n",
    "    main_generated_dataset_size = 0\n",
    "\n",
    "    main_curate_status = \"Curating\"\n",
    "    main_curate_progress_message = \"Starting dataset curation\"\n",
    "    # progress_percentage = \"000.0%\"\n",
    "    # progress_percentage_array = []\n",
    "    main_generate_destination = \"\"\n",
    "    main_initial_bfdataset_size = 0\n",
    "    bf = \"\"\n",
    "    myds = \"\"\n",
    "\n",
    "    main_keys = soda_json_structure.keys()\n",
    "    error = \"\"\n",
    "    \n",
    "    print(soda_json_structure[\"generate-dataset\"])\n",
    "\n",
    "    # 1] Check for potential errors\n",
    "\n",
    "    # 1.1. Check that the local destination is valid if generate dataset locally is requested\n",
    "    if \"generate-dataset\" in main_keys:\n",
    "        if soda_json_structure[\"generate-dataset\"][\"destination\"] == \"local\":\n",
    "            main_curate_progress_message = \"Checking that the local destination selected for generating your dataset is valid\"\n",
    "            generate_dataset = soda_json_structure[\"generate-dataset\"]\n",
    "            local_dataset_path = generate_dataset[\"path\"]\n",
    "            # if generate_dataset[\"if-existing\"] == \"merge\":\n",
    "            #     local_dataset_path = join(local_dataset_path, generate_dataset[\"dataset-name\"])\n",
    "            if not isdir(local_dataset_path):\n",
    "                error_message = (\n",
    "                    \"Error: The Path \"\n",
    "                    + local_dataset_path\n",
    "                    + \" is not found. Please select a valid destination folder for the new dataset\"\n",
    "                )\n",
    "                main_curate_status = \"Done\"\n",
    "                error = error_message\n",
    "                raise Exception(error)\n",
    "\n",
    "    # 1.2. Check that the bf destination is valid if generate on bf, or any other bf actions are requested\n",
    "    if \"bf-account-selected\" in soda_json_structure:\n",
    "        # check that the Pennsieve account is valid\n",
    "        try:\n",
    "            main_curate_progress_message = (\n",
    "                \"Checking that the selected Pennsieve account is valid\"\n",
    "            )\n",
    "            accountname = soda_json_structure[\"bf-account-selected\"][\"account-name\"]\n",
    "            bf = Pennsieve(accountname)\n",
    "        except Exception as e:\n",
    "            main_curate_status = \"Done\"\n",
    "            error = \"Error: Please select a valid Pennsieve account\"\n",
    "            raise Exception(error)\n",
    "\n",
    "    # if uploading on an existing bf dataset\n",
    "    if \"bf-dataset-selected\" in soda_json_structure:\n",
    "        # check that the Pennsieve dataset is valid\n",
    "        try:\n",
    "            main_curate_progress_message = (\n",
    "                \"Checking that the selected Pennsieve dataset is valid\"\n",
    "            )\n",
    "            bfdataset = soda_json_structure[\"bf-dataset-selected\"][\"dataset-name\"]\n",
    "            myds = bf.get_dataset(bfdataset)\n",
    "        except Exception as e:\n",
    "            main_curate_status = \"Done\"\n",
    "            error = \"Error: Please select a valid Pennsieve dataset\"\n",
    "            raise Exception(error)\n",
    "\n",
    "        # check that the user has permissions for uploading and modifying the dataset\n",
    "        try:\n",
    "            main_curate_progress_message = \"Checking that you have required permissions for modifying the selected dataset\"\n",
    "            role = bf_get_current_user_permission(bf, myds)\n",
    "            if role not in [\"owner\", \"manager\", \"editor\"]:\n",
    "                main_curate_status = \"Done\"\n",
    "                error = \"Error: You don't have permissions for uploading to this Pennsieve dataset\"\n",
    "                raise Exception(error)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    # 1.3. Check that specified dataset files and folders are valid (existing path) if generate dataset is requested\n",
    "    # Note: Empty folders and 0 kb files will be removed without warning (a warning will be provided on the front end before starting the curate process)\n",
    "    if \"generate-dataset\" in main_keys:\n",
    "\n",
    "        # Check at least one file or folder are added to the dataset\n",
    "        try:\n",
    "            main_curate_progress_message = \"Checking that the dataset is not empty\"\n",
    "            if (\n",
    "                \"dataset-structure\" not in soda_json_structure\n",
    "                and \"metadata-files\" not in soda_json_structure\n",
    "            ):\n",
    "                main_curate_status = \"Done\"\n",
    "                error = \"Error: Your dataset is empty. Please add valid files and non-empty folders to your dataset.\"\n",
    "                raise Exception(error)\n",
    "        except Exception as e:\n",
    "            main_curate_status = \"Done\"\n",
    "            raise Exception(e)\n",
    "\n",
    "        # Check that local files/folders exist\n",
    "        try:\n",
    "\n",
    "            error = check_local_dataset_files_validity(soda_json_structure)\n",
    "            if error:\n",
    "                main_curate_status = \"Done\"\n",
    "                raise Exception(error)\n",
    "\n",
    "            # check that dataset is not empty after removing all the empty files and folders\n",
    "            if (\n",
    "                not soda_json_structure[\"dataset-structure\"][\"folders\"]\n",
    "                and not \"metadata-files\" in soda_json_structure\n",
    "            ):\n",
    "                main_curate_status = \"Done\"\n",
    "                error = \"Error: Your dataset is empty. Please add valid files and non-empty folders to your dataset.\"\n",
    "                raise Exception(error)\n",
    "\n",
    "        except Exception as e:\n",
    "            main_curate_status = \"Done\"\n",
    "            raise e\n",
    "\n",
    "        # Check that bf files/folders exist\n",
    "        print(soda_json_structure[\"generate-dataset\"])\n",
    "        generate_option = soda_json_structure[\"generate-dataset\"][\"generate-option\"]\n",
    "        print(generate_option)\n",
    "        if generate_option == \"existing-bf\":\n",
    "            try:\n",
    "                main_curate_progress_message = (\n",
    "                    \"Checking that the Pennsieve files and folders are valid\"\n",
    "                )\n",
    "                if soda_json_structure[\"generate-dataset\"][\"destination\"] == \"bf\":\n",
    "                    error = bf_check_dataset_files_validity(soda_json_structure, bf)\n",
    "                    if error:\n",
    "                        main_curate_status = \"Done\"\n",
    "                        raise Exception(error)\n",
    "            except Exception as e:\n",
    "                main_curate_status = \"Done\"\n",
    "                raise e\n",
    "\n",
    "    # 3] Generate\n",
    "    if \"generate-dataset\" in main_keys:\n",
    "        main_curate_progress_message = \"Generating dataset\"\n",
    "        try:\n",
    "            if soda_json_structure[\"generate-dataset\"][\"destination\"] == \"local\":\n",
    "                main_generate_destination = soda_json_structure[\"generate-dataset\"][\n",
    "                    \"destination\"\n",
    "                ]\n",
    "                _, main_total_generate_dataset_size = generate_dataset_locally(\n",
    "                    soda_json_structure\n",
    "                )\n",
    "                # if \"manifest-files\" in main_keys:\n",
    "                #     main_curate_progress_message = \"Generating manifest files\"\n",
    "                #     add_local_manifest_files(manifest_files_structure, datasetpath)\n",
    "\n",
    "            if soda_json_structure[\"generate-dataset\"][\"destination\"] == \"bf\":\n",
    "                main_generate_destination = soda_json_structure[\"generate-dataset\"][\n",
    "                    \"destination\"\n",
    "                ]\n",
    "                if generate_option == \"new\":\n",
    "                    if \"dataset-name\" in soda_json_structure[\"generate-dataset\"]:\n",
    "                        dataset_name = soda_json_structure[\"generate-dataset\"][\n",
    "                            \"dataset-name\"\n",
    "                        ]\n",
    "                        myds = bf_create_new_dataset(dataset_name, bf)\n",
    "                    bf_generate_new_dataset(soda_json_structure, bf, myds)\n",
    "                    # if \"manifest-files\" in main_keys:\n",
    "                    #     main_curate_progress_message = \"Generating manifest files\"\n",
    "                    #     bf_add_manifest_files(manifest_files_structure, myds)\n",
    "                if generate_option == \"existing-bf\":\n",
    "                    myds = bf.get_dataset(bfdataset)\n",
    "                    bf_update_existing_dataset(soda_json_structure, bf, myds)\n",
    "\n",
    "        except Exception as e:\n",
    "            main_curate_status = \"Done\"\n",
    "            raise e\n",
    "\n",
    "    main_curate_status = \"Done\"\n",
    "    main_curate_progress_message = \"Success: COMPLETED!\"\n",
    "\n",
    "    return main_curate_progress_message, main_total_generate_dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_update_existing_dataset(soda_json_structure, bf, ds):\n",
    "    global main_curate_progress_message\n",
    "    global main_total_generate_dataset_size\n",
    "    global start_generate\n",
    "    global main_initial_bfdataset_size\n",
    "    # global progress_percentage\n",
    "    # global progress_percentage_array\n",
    "    bfsd = \"\"\n",
    "\n",
    "    # Delete any files on Pennsieve that have been marked as deleted\n",
    "    def recursive_file_delete(folder):\n",
    "        if \"files\" in folder.keys():\n",
    "            for item in list(folder[\"files\"]):\n",
    "                if \"deleted\" in folder[\"files\"][item][\"action\"]:\n",
    "                    file = bf.get(folder[\"files\"][item][\"path\"])\n",
    "                    file.delete()\n",
    "                    del folder[\"files\"][item]\n",
    "\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            recursive_file_delete(folder[\"folders\"][item])\n",
    "        return\n",
    "\n",
    "    # Delete any files on Pennsieve that have been marked as deleted\n",
    "    def metadata_file_delete(soda_json_structure):\n",
    "        if \"metadata-files\" in soda_json_structure.keys():\n",
    "            folder = soda_json_structure[\"metadata-files\"]\n",
    "            for item in list(folder):\n",
    "                if \"deleted\" in folder[item][\"action\"]:\n",
    "                    file = bf.get(folder[item][\"path\"])\n",
    "                    file.delete()\n",
    "                    del folder[item]\n",
    "\n",
    "        return\n",
    "\n",
    "    # Add a new key containing the path to all the files and folders on the\n",
    "    # local data structure.\n",
    "    # Allows us to see if the folder path of a specfic file already\n",
    "    # exists on Pennsieve.\n",
    "    def recursive_item_path_create(folder, path):\n",
    "        if \"files\" in folder.keys():\n",
    "            for item in list(folder[\"files\"]):\n",
    "                if \"folderpath\" not in folder[\"files\"][item]:\n",
    "                    folder[\"files\"][item][\"folderpath\"] = path[:]\n",
    "\n",
    "        if \"folders\" in folder.keys():\n",
    "            for item in list(folder[\"folders\"]):\n",
    "                if \"folderpath\" not in folder[\"folders\"][item]:\n",
    "                    folder[\"folders\"][item][\"folderpath\"] = path[:]\n",
    "                    folder[\"folders\"][item][\"folderpath\"].append(item)\n",
    "                recursive_item_path_create(\n",
    "                    folder[\"folders\"][item], folder[\"folders\"][item][\"folderpath\"][:]\n",
    "                )\n",
    "\n",
    "        return\n",
    "\n",
    "    # Check and create any non existing folders for the file move process\n",
    "    def recursive_check_and_create_bf_file_path(\n",
    "        folderpath, index, current_folder_structure\n",
    "    ):\n",
    "        folder = folderpath[index]\n",
    "\n",
    "        if folder not in current_folder_structure[\"folders\"]:\n",
    "            if index == 0:\n",
    "                new_folder = ds.create_collection(folder)\n",
    "            else:\n",
    "                current_folder = bf.get(current_folder_structure[\"path\"])\n",
    "                new_folder = current_folder.create_collection(folder)\n",
    "            current_folder_structure[\"folders\"][folder] = {\n",
    "                \"type\": \"bf\",\n",
    "                \"action\": [\"existing\"],\n",
    "                \"path\": new_folder.id,\n",
    "                \"folders\": {},\n",
    "                \"files\": {},\n",
    "            }\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        if index < len(folderpath):\n",
    "            return recursive_check_and_create_bf_file_path(\n",
    "                folderpath, index, current_folder_structure[\"folders\"][folder]\n",
    "            )\n",
    "        else:\n",
    "            return current_folder_structure[\"folders\"][folder][\"path\"]\n",
    "\n",
    "    # Check for any files that have been moved and verify paths before moving\n",
    "    def recursive_check_moved_files(folder):\n",
    "        if \"files\" in folder.keys():\n",
    "            for item in list(folder[\"files\"]):\n",
    "                if (\n",
    "                    \"moved\" in folder[\"files\"][item][\"action\"]\n",
    "                    and folder[\"files\"][item][\"type\"] == \"bf\"\n",
    "                ):\n",
    "                    new_folder_id = \"\"\n",
    "                    new_folder_id = recursive_check_and_create_bf_file_path(\n",
    "                        folder[\"files\"][item][\"folderpath\"].copy(), 0, bfsd\n",
    "                    )\n",
    "                    destination_folder = bf.get(new_folder_id)\n",
    "                    bf.move(destination_folder, folder[\"files\"][item][\"path\"])\n",
    "\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            recursive_check_moved_files(folder[\"folders\"][item])\n",
    "\n",
    "        return\n",
    "\n",
    "    # Rename any files that exist on Pennsieve\n",
    "    def recursive_file_rename(folder):\n",
    "        if \"files\" in folder.keys():\n",
    "            for item in list(folder[\"files\"]):\n",
    "                if (\n",
    "                    \"renamed\" in folder[\"files\"][item][\"action\"]\n",
    "                    and folder[\"files\"][item][\"type\"] == \"bf\"\n",
    "                ):\n",
    "                    file = bf.get(folder[\"files\"][item][\"path\"])\n",
    "                    if file is not None:\n",
    "                        file.name = item\n",
    "                        file.update()\n",
    "\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            recursive_file_rename(folder[\"folders\"][item])\n",
    "\n",
    "        return\n",
    "\n",
    "    # Delete any stray folders that exist on Pennsieve\n",
    "    # Only top level files are deleted since the api deletes any\n",
    "    # files and folders that exist inside.\n",
    "    def recursive_folder_delete(folder):\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            if folder[\"folders\"][item][\"type\"] == \"bf\":\n",
    "                if \"moved\" in folder[\"folders\"][item][\"action\"]:\n",
    "                    file = bf.get(folder[\"folders\"][item][\"path\"])\n",
    "                    if file is not None:\n",
    "                        file.delete()\n",
    "                if \"deleted\" in folder[\"folders\"][item][\"action\"]:\n",
    "                    file = bf.get(folder[\"folders\"][item][\"path\"])\n",
    "                    if file is not None:\n",
    "                        file.delete()\n",
    "                    del folder[\"folders\"][item]\n",
    "                else:\n",
    "                    recursive_folder_delete(folder[\"folders\"][item])\n",
    "            else:\n",
    "                recursive_folder_delete(folder[\"folders\"][item])\n",
    "\n",
    "        return\n",
    "\n",
    "    # Rename any folders that still exist.\n",
    "    def recursive_folder_rename(folder, mode):\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            if (\n",
    "                folder[\"folders\"][item][\"type\"] == \"bf\"\n",
    "                and \"action\" in folder[\"folders\"][item].keys()\n",
    "            ):\n",
    "                if mode in folder[\"folders\"][item][\"action\"]:\n",
    "                    file = bf.get(folder[\"folders\"][item][\"path\"])\n",
    "                    if file is not None:\n",
    "                        file.name = item\n",
    "                        file.update()\n",
    "            recursive_folder_rename(folder[\"folders\"][item], mode)\n",
    "\n",
    "        return\n",
    "\n",
    "    # 1. Remove all existing files on Pennsieve, that the user deleted.\n",
    "    main_curate_progress_message = \"Checking Pennsieve for deleted files\"\n",
    "    dataset_structure = soda_json_structure[\"dataset-structure\"]\n",
    "    recursive_file_delete(dataset_structure)\n",
    "    main_curate_progress_message = (\n",
    "        \"Files on Pennsieve marked for deletion have been deleted\"\n",
    "    )\n",
    "\n",
    "    # 2. Rename any deleted folders on Pennsieve to allow for replacements.\n",
    "    main_curate_progress_message = \"Checking Pennsieve for deleted folders\"\n",
    "    dataset_structure = soda_json_structure[\"dataset-structure\"]\n",
    "    recursive_folder_rename(dataset_structure, \"deleted\")\n",
    "    main_curate_progress_message = \"Folders on Pennsieve have been marked for deletion\"\n",
    "\n",
    "    # 2.5 Rename folders that need to be in the final destination.\n",
    "    main_curate_progress_message = \"Renaming any folders requested by the user\"\n",
    "    recursive_folder_rename(dataset_structure, \"renamed\")\n",
    "    main_curate_progress_message = \"Renamed all folders requested by the user\"\n",
    "\n",
    "    # 3. Get the status of all files currently on Pennsieve and create\n",
    "    # the folderpath for all items in both dataset structures.\n",
    "    main_curate_progress_message = \"Fetching files and folders from Pennsieve\"\n",
    "    current_bf_dataset_files_folders = bf_get_dataset_files_folders(\n",
    "        soda_json_structure.copy()\n",
    "    )[0]\n",
    "    bfsd = current_bf_dataset_files_folders[\"dataset-structure\"]\n",
    "    main_curate_progress_message = \"Creating file paths for all files on Pennsieve\"\n",
    "    recursive_item_path_create(dataset_structure, [])\n",
    "    recursive_item_path_create(bfsd, [])\n",
    "    main_curate_progress_message = \"File paths created\"\n",
    "\n",
    "    # 4. Move any files that are marked as moved on Pennsieve.\n",
    "    # Create any additional folders if required\n",
    "    main_curate_progress_message = \"Moving any files requested by the user\"\n",
    "    recursive_check_moved_files(dataset_structure)\n",
    "    main_curate_progress_message = \"Moved all files requested by the user\"\n",
    "\n",
    "    # 5. Rename any Pennsieve files that are marked as renamed.\n",
    "    main_curate_progress_message = \"Renaming any files requested by the user\"\n",
    "    recursive_file_rename(dataset_structure)\n",
    "    main_curate_progress_message = \"Renamed all files requested by the user\"\n",
    "\n",
    "    # 6. Delete any Pennsieve folders that are marked as deleted.\n",
    "    main_curate_progress_message = (\n",
    "        \"Deleting any additional folders present on Pennsieve\"\n",
    "    )\n",
    "    recursive_folder_delete(dataset_structure)\n",
    "    main_curate_progress_message = \"Deletion of additional folders complete\"\n",
    "\n",
    "    # 7. Rename any Pennsieve folders that are marked as renamed.\n",
    "    main_curate_progress_message = \"Renaming any folders requested by the user\"\n",
    "    recursive_folder_rename(dataset_structure, \"renamed\")\n",
    "    main_curate_progress_message = \"Renamed all folders requested by the user\"\n",
    "\n",
    "    # 8. Delete any metadata files that are marked as deleted.\n",
    "    main_curate_progress_message = \"Removing any metadata files marked for deletion\"\n",
    "    metadata_file_delete(soda_json_structure)\n",
    "    main_curate_progress_message = \"Removed metadata files marked for deletion\"\n",
    "\n",
    "    # 9. Run the original code to upload any new files added to the dataset.\n",
    "    if \"manifest-files\" in soda_json_structure.keys():\n",
    "        soda_json_structure[\"manifest-files\"] = {\"destination\": \"bf\"}\n",
    "\n",
    "    soda_json_structure[\"generate-dataset\"] = {\n",
    "        \"destination\": \"bf\",\n",
    "        \"if-existing\": \"merge\",\n",
    "        \"if-existing-files\": \"replace\",\n",
    "    }\n",
    "\n",
    "    bfdataset = soda_json_structure[\"bf-dataset-selected\"][\"dataset-name\"]\n",
    "    myds = bf.get_dataset(bfdataset)\n",
    "    bf_generate_new_dataset(soda_json_structure, bf, myds)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_generate_new_dataset(soda_json_structure, bf, ds):\n",
    "\n",
    "    global main_curate_progress_message\n",
    "    global main_total_generate_dataset_size\n",
    "    global start_generate\n",
    "    global main_initial_bfdataset_size\n",
    "    # global progress_percentage\n",
    "    # global progress_percentage_array\n",
    "\n",
    "    try:\n",
    "\n",
    "        def recursive_create_folder_for_bf(\n",
    "            my_folder, my_tracking_folder, existing_folder_option\n",
    "        ):\n",
    "\n",
    "            # list of existing bf folders at this level\n",
    "            my_bf_folder = my_tracking_folder[\"value\"]\n",
    "            (\n",
    "                my_bf_existing_folders,\n",
    "                my_bf_existing_folders_name,\n",
    "            ) = bf_get_existing_folders_details(my_bf_folder)\n",
    "\n",
    "            # create/replace/skip folder\n",
    "            if \"folders\" in my_folder.keys():\n",
    "                my_tracking_folder[\"folders\"] = {}\n",
    "                for folder_key, folder in my_folder[\"folders\"].items():\n",
    "\n",
    "                    if existing_folder_option == \"skip\":\n",
    "                        if folder_key in my_bf_existing_folders_name:\n",
    "                            continue\n",
    "                        else:\n",
    "                            bf_folder = my_bf_folder.create_collection(folder_key)\n",
    "\n",
    "                    elif existing_folder_option == \"create-duplicate\":\n",
    "                        bf_folder = my_bf_folder.create_collection(folder_key)\n",
    "\n",
    "                    elif existing_folder_option == \"replace\":\n",
    "                        if folder_key in my_bf_existing_folders_name:\n",
    "                            index_folder = my_bf_existing_folders_name.index(folder_key)\n",
    "                            bf_folder_delete = my_bf_existing_folders[index_folder]\n",
    "                            bf_folder_delete.delete()\n",
    "                            my_bf_folder.update()\n",
    "                        bf_folder = my_bf_folder.create_collection(folder_key)\n",
    "\n",
    "                    elif existing_folder_option == \"merge\":\n",
    "                        if folder_key in my_bf_existing_folders_name:\n",
    "                            index_folder = my_bf_existing_folders_name.index(folder_key)\n",
    "                            bf_folder = my_bf_existing_folders[index_folder]\n",
    "                        else:\n",
    "                            bf_folder = my_bf_folder.create_collection(folder_key)\n",
    "                    bf_folder.update()\n",
    "                    my_tracking_folder[\"folders\"][folder_key] = {\"value\": bf_folder}\n",
    "                    tracking_folder = my_tracking_folder[\"folders\"][folder_key]\n",
    "                    recursive_create_folder_for_bf(\n",
    "                        folder, tracking_folder, existing_folder_option\n",
    "                    )\n",
    "\n",
    "        def recursive_dataset_scan_for_bf(\n",
    "            my_folder,\n",
    "            my_tracking_folder,\n",
    "            existing_file_option,\n",
    "            list_upload_files,\n",
    "            my_relative_path,\n",
    "        ):\n",
    "\n",
    "            global main_total_generate_dataset_size\n",
    "\n",
    "            my_bf_folder = my_tracking_folder[\"value\"]\n",
    "\n",
    "            if \"folders\" in my_folder.keys():\n",
    "                (\n",
    "                    my_bf_existing_folders,\n",
    "                    my_bf_existing_folders_name,\n",
    "                ) = bf_get_existing_folders_details(my_bf_folder)\n",
    "\n",
    "                for folder_key, folder in my_folder[\"folders\"].items():\n",
    "                    relative_path = generate_relative_path(my_relative_path, folder_key)\n",
    "\n",
    "                    if existing_folder_option == \"skip\":\n",
    "                        if folder_key not in my_tracking_folder[\"folders\"].keys():\n",
    "                            continue\n",
    "\n",
    "                    tracking_folder = my_tracking_folder[\"folders\"][folder_key]\n",
    "                    list_upload_files = recursive_dataset_scan_for_bf(\n",
    "                        folder,\n",
    "                        tracking_folder,\n",
    "                        existing_file_option,\n",
    "                        list_upload_files,\n",
    "                        relative_path,\n",
    "                    )\n",
    "\n",
    "            if \"files\" in my_folder.keys():\n",
    "\n",
    "                # delete files to be deleted\n",
    "                (\n",
    "                    my_bf_existing_files,\n",
    "                    my_bf_existing_files_name,\n",
    "                    my_bf_existing_files_name_with_extension,\n",
    "                ) = bf_get_existing_files_details(my_bf_folder)\n",
    "                for file_key, file in my_folder[\"files\"].items():\n",
    "                    if file[\"type\"] == \"local\":\n",
    "                        file_path = file[\"path\"]\n",
    "                        if isfile(file_path):\n",
    "                            if existing_file_option == \"replace\":\n",
    "                                if file_key in my_bf_existing_files_name_with_extension:\n",
    "                                    index_file = (\n",
    "                                        my_bf_existing_files_name_with_extension.index(\n",
    "                                            file_key\n",
    "                                        )\n",
    "                                    )\n",
    "                                    my_file = my_bf_existing_files[index_file]\n",
    "                                    my_file.delete()\n",
    "                                    my_bf_folder.update()\n",
    "\n",
    "                # create list of files to be uploaded with projected and desired names saved\n",
    "                (\n",
    "                    my_bf_existing_files,\n",
    "                    my_bf_existing_files_name,\n",
    "                    my_bf_existing_files_name_with_extension,\n",
    "                ) = bf_get_existing_files_details(my_bf_folder)\n",
    "\n",
    "                list_local_files = []\n",
    "                list_projected_names = []\n",
    "                list_desired_names = []\n",
    "                list_final_names = []\n",
    "                additional_upload_lists = []\n",
    "                additional_list_count = 0\n",
    "                list_upload_schedule_projected_names = []\n",
    "                list_initial_names = []\n",
    "                for file_key, file in my_folder[\"files\"].items():\n",
    "                    if file[\"type\"] == \"local\":\n",
    "                        file_path = file[\"path\"]\n",
    "                        if isfile(file_path):\n",
    "                            initial_name = splitext(basename(file_path))[0]\n",
    "                            initial_extension = splitext(basename(file_path))[1]\n",
    "                            initial_name_with_extension = basename(file_path)\n",
    "                            desired_name = splitext(file_key)[0]\n",
    "                            desired_name_extension = splitext(file_key)[1]\n",
    "                            desired_name_with_extension = file_key\n",
    "                            # desired_name = file_key\n",
    "\n",
    "                            if existing_file_option == \"skip\":\n",
    "                                if file_key in my_bf_existing_files_name_with_extension:\n",
    "                                    continue\n",
    "\n",
    "                            # check if initial filename exists on Pennsieve dataset and get the projected name of the file after upload\n",
    "                            count_done = 0\n",
    "                            count_exist = 0\n",
    "                            projected_name = initial_name_with_extension\n",
    "                            while count_done == 0:\n",
    "                                if (\n",
    "                                    projected_name\n",
    "                                    in my_bf_existing_files_name_with_extension\n",
    "                                ):\n",
    "                                    count_exist += 1\n",
    "                                    projected_name = (\n",
    "                                        initial_name\n",
    "                                        + \" (\"\n",
    "                                        + str(count_exist)\n",
    "                                        + \")\"\n",
    "                                        + initial_extension\n",
    "                                    )\n",
    "                                else:\n",
    "                                    count_done = 1\n",
    "\n",
    "                            # expected final name\n",
    "                            count_done = 0\n",
    "                            final_name = desired_name_with_extension\n",
    "                            output = get_base_file_name(desired_name)\n",
    "                            if output:\n",
    "                                base_name = output[0]\n",
    "                                count_exist = output[1]\n",
    "                                while count_done == 0:\n",
    "                                    if final_name in my_bf_existing_files_name:\n",
    "                                        count_exist += 1\n",
    "                                        final_name = (\n",
    "                                            base_name\n",
    "                                            + \"(\"\n",
    "                                            + str(count_exist)\n",
    "                                            + \")\"\n",
    "                                            + desired_name_extension\n",
    "                                        )\n",
    "                                    else:\n",
    "                                        count_done = 1\n",
    "                            else:\n",
    "                                count_exist = 0\n",
    "                                while count_done == 0:\n",
    "                                    if final_name in my_bf_existing_files_name:\n",
    "                                        count_exist += 1\n",
    "                                        final_name = (\n",
    "                                            desired_name\n",
    "                                            + \" (\"\n",
    "                                            + str(count_exist)\n",
    "                                            + \")\"\n",
    "                                            + desired_name_extension\n",
    "                                        )\n",
    "                                    else:\n",
    "                                        count_done = 1\n",
    "\n",
    "                            # save in list accordingly\n",
    "                            if (\n",
    "                                initial_name in list_initial_names\n",
    "                                or initial_name in list_final_names\n",
    "                                or projected_name in list_final_names\n",
    "                                or final_name in list_projected_names\n",
    "                            ):\n",
    "                                additional_upload_lists.append(\n",
    "                                    [\n",
    "                                        [file_path],\n",
    "                                        my_bf_folder,\n",
    "                                        [projected_name],\n",
    "                                        [desired_name],\n",
    "                                        [final_name],\n",
    "                                        my_tracking_folder,\n",
    "                                        my_relative_path,\n",
    "                                    ]\n",
    "                                )\n",
    "                            else:\n",
    "                                list_local_files.append(file_path)\n",
    "                                list_projected_names.append(projected_name)\n",
    "                                list_desired_names.append(desired_name_with_extension)\n",
    "                                list_final_names.append(final_name)\n",
    "                                list_initial_names.append(initial_name)\n",
    "\n",
    "                            my_bf_existing_files_name.append(final_name)\n",
    "                            if initial_extension in bf_recognized_file_extensions:\n",
    "                                my_bf_existing_files_name_with_extension.append(\n",
    "                                    final_name\n",
    "                                )\n",
    "                            else:\n",
    "                                my_bf_existing_files_name_with_extension.append(\n",
    "                                    final_name + initial_extension\n",
    "                                )\n",
    "\n",
    "                            # add to projected dataset size to be generated\n",
    "                            main_total_generate_dataset_size += getsize(file_path)\n",
    "\n",
    "                if list_local_files:\n",
    "                    list_upload_files.append(\n",
    "                        [\n",
    "                            list_local_files,\n",
    "                            my_bf_folder,\n",
    "                            list_projected_names,\n",
    "                            list_desired_names,\n",
    "                            list_final_names,\n",
    "                            my_tracking_folder,\n",
    "                            my_relative_path,\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                for item in additional_upload_lists:\n",
    "                    list_upload_files.append(item)\n",
    "\n",
    "            return list_upload_files\n",
    "\n",
    "        # 1. Scan the dataset structure to create all non-existent folders\n",
    "        # create a tracking dict which would track the generation of the dataset on Pennsieve\n",
    "        main_curate_progress_message = \"Creating folder structure\"\n",
    "        dataset_structure = soda_json_structure[\"dataset-structure\"]\n",
    "        tracking_json_structure = {\"value\": ds}\n",
    "        existing_folder_option = soda_json_structure[\"generate-dataset\"][\"if-existing\"]\n",
    "        recursive_create_folder_for_bf(\n",
    "            dataset_structure, tracking_json_structure, existing_folder_option\n",
    "        )\n",
    "\n",
    "        # 2. Scan the dataset structure and compile a list of files to be uploaded along with desired renaming\n",
    "        ds.update()\n",
    "        main_curate_progress_message = \"Preparing a list of files to upload\"\n",
    "        existing_file_option = soda_json_structure[\"generate-dataset\"][\n",
    "            \"if-existing-files\"\n",
    "        ]\n",
    "        list_upload_files = []\n",
    "        relative_path = ds.name\n",
    "        list_upload_files = recursive_dataset_scan_for_bf(\n",
    "            dataset_structure,\n",
    "            tracking_json_structure,\n",
    "            existing_file_option,\n",
    "            list_upload_files,\n",
    "            relative_path,\n",
    "        )\n",
    "\n",
    "        # 3. Add high-level metadata files to a list\n",
    "        ds.update()\n",
    "        list_upload_metadata_files = []\n",
    "        if \"metadata-files\" in soda_json_structure.keys():\n",
    "\n",
    "            (\n",
    "                my_bf_existing_files,\n",
    "                my_bf_existing_files_name,\n",
    "                my_bf_existing_files_name_with_extension,\n",
    "            ) = bf_get_existing_files_details(ds)\n",
    "            metadata_files = soda_json_structure[\"metadata-files\"]\n",
    "            for file_key, file in metadata_files.items():\n",
    "                if file[\"type\"] == \"local\":\n",
    "                    metadata_path = file[\"path\"]\n",
    "                    if isfile(metadata_path):\n",
    "                        initial_name = splitext(basename(metadata_path))[0]\n",
    "                        if existing_file_option == \"replace\":\n",
    "                            if initial_name in my_bf_existing_files_name:\n",
    "                                index_file = my_bf_existing_files_name.index(\n",
    "                                    initial_name\n",
    "                                )\n",
    "                                my_file = my_bf_existing_files[index_file]\n",
    "                                my_file.delete()\n",
    "\n",
    "                        if existing_file_option == \"skip\":\n",
    "                            if initial_name in my_bf_existing_files_name:\n",
    "                                continue\n",
    "\n",
    "                        list_upload_metadata_files.append(metadata_path)\n",
    "                        main_total_generate_dataset_size += getsize(metadata_path)\n",
    "\n",
    "        # 4. Prepare and add manifest files to a list\n",
    "        list_upload_manifest_files = []\n",
    "        if \"manifest-files\" in soda_json_structure.keys():\n",
    "\n",
    "            # prepare manifest files\n",
    "            if soda_json_structure[\"starting-point\"][\"type\"] == \"bf\":\n",
    "                manifest_files_structure = (\n",
    "                    create_high_level_manifest_files_existing_bf_starting_point(\n",
    "                        soda_json_structure\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                if (\n",
    "                    soda_json_structure[\"generate-dataset\"][\"destination\"] == \"bf\"\n",
    "                    and \"dataset-name\" not in soda_json_structure[\"generate-dataset\"]\n",
    "                ):\n",
    "                    # generating dataset on an existing bf dataset - account for existing files and manifest files\n",
    "                    manifest_files_structure = (\n",
    "                        create_high_level_manifest_files_existing_bf(\n",
    "                            soda_json_structure, bf, ds, tracking_json_structure\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    # generating on new bf\n",
    "                    manifest_files_structure = create_high_level_manifest_files(\n",
    "                        soda_json_structure\n",
    "                    )\n",
    "\n",
    "            # add manifest files to list after deleting existing ones\n",
    "            list_upload_manifest_files = []\n",
    "            for key in manifest_files_structure.keys():\n",
    "                manifestpath = manifest_files_structure[key]\n",
    "                item = tracking_json_structure[\"folders\"][key][\"value\"]\n",
    "                destination_folder_id = item.id\n",
    "                # delete existing manifest files\n",
    "                for subitem in item:\n",
    "                    file_name_no_ext = os.path.splitext(subitem.name)[0]\n",
    "                    if file_name_no_ext.lower() == \"manifest\":\n",
    "                        subitem.delete()\n",
    "                        item.update()\n",
    "                # upload new manifest files\n",
    "                list_upload_manifest_files.append([[manifestpath], item])\n",
    "                main_total_generate_dataset_size += getsize(manifestpath)\n",
    "\n",
    "        # clear the pennsieve queue\n",
    "        clear_queue()\n",
    "        \n",
    "       #  print(\"Pennsieve queue cleared\")\n",
    "        print(\"\\n\")\n",
    "        print(\"List upload files all together: \", list_upload_files)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # 5. Upload files, rename, and add to tracking list\n",
    "        main_initial_bfdataset_size = bf_dataset_size()\n",
    "        start_generate = 1\n",
    "        for item in list_upload_files:\n",
    "            list_upload = item[0]\n",
    "            print(\"list upload item: \", list_upload)\n",
    "            print(\"\\n\")\n",
    "            bf_folder = item[1]\n",
    "            list_projected_names = item[2]\n",
    "            print(\"The list of projected names is: \", list_projected_names)\n",
    "            print(\"\\n\")\n",
    "            list_desired_names = item[3]\n",
    "            print(\"The list of desired names is: \", list_desired_names)\n",
    "            print(\"\\n\")\n",
    "            list_final_names = item[4]\n",
    "            print(\"The list of final names is: \", list_final_names)\n",
    "            print(\"\\n\")\n",
    "            tracking_folder = item[5]\n",
    "            relative_path = item[6]\n",
    "            \n",
    "            ## check if agent is running in the background\n",
    "            agent_running()\n",
    "            \n",
    "            # determine if the current folder's files exceeds 750 (past 750 is a breaking point atm)\n",
    "            # if so proceed to batch uploading \n",
    "            if list_upload.length > 750:\n",
    "                print(\"Did not execute bucketing\")\n",
    "                # store the aggregate of the amount of files in the folder\n",
    "                total_files = list_upload.length\n",
    "                \n",
    "                # create a start index and an end index\n",
    "                start_index = end_index =  0\n",
    "                \n",
    "                # while startIndex < files.length \n",
    "                while startIndex < total_files:\n",
    "                    # set the endIndex to startIndex plus 750\n",
    "                    end_index = start_index + 749\n",
    "                    \n",
    "                    # check if the endIndex is out of bounds\n",
    "                    if end_index >= total_files: \n",
    "                        # if so set end index to files.length - 1\n",
    "                        end_index = list_upload.length - 1\n",
    "                        \n",
    "                    # get the 750 files between startIndex and endIndex (inclusive of endIndex)\n",
    "                    upload_bucket = list_upload[start_index: end_index + 1]\n",
    "                    \n",
    "                    # inform the user files are being uploaded\n",
    "                    main_curate_progress_message = \"Uploading files in \" + str(relative_path)\n",
    "                    \n",
    "                    # clear the pennsieve queue for successive batches\n",
    "                    clear_queue()\n",
    "                    \n",
    "                    # upload the files\n",
    "                    bf_folder.upload(*upload_bucket)\n",
    "           \n",
    "                    # update the files \n",
    "                    bf_folder.update()\n",
    "                \n",
    "                    # handle renaming to final names\n",
    "                    for index, projected_name in enumerate(list_projected_names[start_index:end_index + 1]):\n",
    "                        final_name = list_final_names[index]\n",
    "                        desired_name = list_desired_names[index]\n",
    "                        if final_name != projected_name:\n",
    "                            bf_item_list = bf_folder.items\n",
    "                            (\n",
    "                                my_bf_existing_files,\n",
    "                                my_bf_existing_files_name,\n",
    "                                my_bf_existing_files_name_with_extension,\n",
    "                            ) = bf_get_existing_files_details(bf_folder)\n",
    "                            for item in my_bf_existing_files:\n",
    "                                if item.name == projected_name:\n",
    "                                    item.name = final_name\n",
    "                                    item.update()\n",
    "                                    if \"files\" not in tracking_folder:\n",
    "                                        tracking_folder[\"files\"] = {}\n",
    "                                        tracking_folder[\"files\"][desired_name] = {\"value\": item}\n",
    "                 \n",
    "                # update the start_index to end_index + 1\n",
    "                start_index = end_index + 1      \n",
    "            else:\n",
    "                # upload all files at once for the folder \n",
    "                main_curate_progress_message = \"Uploading files in \" + str(relative_path)\n",
    "                print(main_curate_progress_message)\n",
    "                print(\"\\n\")\n",
    "\n",
    "                # fails when a single folder has more than 750 files (at which point I'm not sure)\n",
    "                bf_folder.upload(*list_upload)\n",
    "                bf_folder.update()\n",
    "\n",
    "                # rename to final name\n",
    "                for index, projected_name in enumerate(list_projected_names):\n",
    "                    final_name = list_final_names[index]\n",
    "                    desired_name = list_desired_names[index]\n",
    "                    if final_name != projected_name:\n",
    "                        bf_item_list = bf_folder.items\n",
    "                        (\n",
    "                            my_bf_existing_files,\n",
    "                            my_bf_existing_files_name,\n",
    "                            my_bf_existing_files_name_with_extension,\n",
    "                        ) = bf_get_existing_files_details(bf_folder)\n",
    "                        for item in my_bf_existing_files:\n",
    "                            if item.name == projected_name:\n",
    "                                item.name = final_name\n",
    "                                item.update()\n",
    "                                if \"files\" not in tracking_folder:\n",
    "                                    tracking_folder[\"files\"] = {}\n",
    "                                    tracking_folder[\"files\"][desired_name] = {\"value\": item}\n",
    "\n",
    "        if list_upload_metadata_files:\n",
    "            main_curate_progress_message = (\n",
    "                \"Uploading metadata files in high-level dataset folder \" + str(ds.name)\n",
    "            )\n",
    "            ds.upload(*list_upload_metadata_files)\n",
    "\n",
    "        if list_upload_manifest_files:\n",
    "            for item in list_upload_manifest_files:\n",
    "                manifest_file = item[0]\n",
    "                bf_folder = item[1]\n",
    "                main_curate_progress_message = (\n",
    "                    \"Uploading manifest file in \" + str(bf_folder.name) + \" folder\"\n",
    "                )\n",
    "                bf_folder.upload(*manifest_file)\n",
    "                # bf_folder.update()\n",
    "        shutil.rmtree(manifest_folder_path) if isdir(manifest_folder_path) else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'destination': 'bf', 'generate-option': 'existing-bf'}\n",
      "{'destination': 'bf', 'generate-option': 'existing-bf'}\n",
      "existing-bf\n",
      "\n",
      "\n",
      "List upload files all together:  [[['C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\summary.csv', 'C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\term.csv'], <Collection name='sub-1-primary' id='N:collection:0d899fd6-291b-43ea-b8f7-7183d32d7ee3'>, ['summary.csv', 'term.csv'], ['summary.csv', 'term.csv'], ['summary.csv', 'term.csv'], {'value': <Collection name='sub-1-primary' id='N:collection:0d899fd6-291b-43ea-b8f7-7183d32d7ee3'>, 'folders': {}}, 'chex/primary/sub-1-primary'], [['C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\summary.csv', 'C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\term.csv'], <Collection name='primary' id='N:collection:3b43dc92-1394-482d-a2eb-fe319eba454d'>, ['summary.csv', 'term.csv'], ['summary.csv', 'term.csv'], ['summary.csv', 'term.csv'], {'value': <Collection name='primary' id='N:collection:3b43dc92-1394-482d-a2eb-fe319eba454d'>, 'folders': {'sub-1-primary': {'value': <Collection name='sub-1-primary' id='N:collection:0d899fd6-291b-43ea-b8f7-7183d32d7ee3'>, 'folders': {}}}}, 'chex/primary'], [['C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\file.csv', 'C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\human_subject.csv', 'C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\protocol.csv', 'C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\researcher.csv'], <Collection name='sub-1-source' id='N:collection:f0cbe015-6c27-4d09-a3a7-0cac02c4b0dd'>, ['file.csv', 'human_subject.csv', 'protocol.csv', 'researcher.csv'], ['file.csv', 'human_subject.csv', 'protocol.csv', 'researcher.csv'], ['file.csv', 'human_subject.csv', 'protocol.csv', 'researcher.csv'], {'value': <Collection name='sub-1-source' id='N:collection:f0cbe015-6c27-4d09-a3a7-0cac02c4b0dd'>, 'folders': {}}, 'chex/source/sub-1-source'], [['C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\protocol.csv', 'C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\award.csv'], <Collection name='source' id='N:collection:9ec737b3-9739-4bc3-8a93-fcb588edfbbc'>, ['protocol.csv', 'award.csv'], ['protocol.csv', 'award.csv'], ['protocol.csv', 'award.csv'], {'value': <Collection name='source' id='N:collection:9ec737b3-9739-4bc3-8a93-fcb588edfbbc'>, 'folders': {'sub-1-source': {'value': <Collection name='sub-1-source' id='N:collection:f0cbe015-6c27-4d09-a3a7-0cac02c4b0dd'>, 'folders': {}}}}, 'chex/source'], [['C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\samples.csv'], <Collection name='protocol' id='N:collection:16fbda5e-c098-4763-9938-9cf682525ae4'>, ['samples.csv'], ['samples.csv'], ['samples.csv'], {'value': <Collection name='protocol' id='N:collection:16fbda5e-c098-4763-9938-9cf682525ae4'>, 'folders': {}}, 'chex/protocol']]\n",
      "\n",
      "\n",
      "list upload item:  ['C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\summary.csv', 'C:\\\\Users\\\\CMarroquin\\\\OneDrive - Calmi2\\\\Documents\\\\Pennsieve-dataset-114-version-2\\\\metadata\\\\records\\\\term.csv']\n",
      "\n",
      "\n",
      "The list of projected names is:  ['summary.csv', 'term.csv']\n",
      "\n",
      "\n",
      "The list of desired names is:  ['summary.csv', 'term.csv']\n",
      "\n",
      "\n",
      "The list of final names is:  ['summary.csv', 'term.csv']\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'socket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18292\\2528650457.py\u001b[0m in \u001b[0;36magent_running\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# CHANGE BACK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mcreate_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket_address\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlisten_port\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_connection' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18292\\3719066163.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# run the dataset updater function with the given soda json object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmain_curate_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msodaJSONObject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18292\\1334820921.py\u001b[0m in \u001b[0;36mmain_curate_function\u001b[1;34m(soda_json_structure)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mmain_curate_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Done\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mmain_curate_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Done\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18292\\1334820921.py\u001b[0m in \u001b[0;36mmain_curate_function\u001b[1;34m(soda_json_structure)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgenerate_option\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"existing-bf\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mmyds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbfdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                     \u001b[0mbf_update_existing_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoda_json_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18292\\2586102278.py\u001b[0m in \u001b[0;36mbf_update_existing_dataset\u001b[1;34m(soda_json_structure, bf, ds)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[0mbfdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoda_json_structure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bf-dataset-selected\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dataset-name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[0mmyds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbfdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mbf_generate_new_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoda_json_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18292\\3024704106.py\u001b[0m in \u001b[0;36mbf_generate_new_dataset\u001b[1;34m(soda_json_structure, bf, ds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18292\\3024704106.py\u001b[0m in \u001b[0;36mbf_generate_new_dataset\u001b[1;34m(soda_json_structure, bf, ds)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m## check if agent is running in the background\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0magent_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[1;31m# determine if the current folder's files exceeds 750 (past 750 is a breaking point atm)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18292\\2528650457.py\u001b[0m in \u001b[0;36magent_running\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mcreate_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket_address\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlisten_port\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mECONNREFUSED\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# ConnectionRefusedError for Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'socket' is not defined"
     ]
    }
   ],
   "source": [
    "# run the dataset updater function with the given soda json object \n",
    "main_curate_function(sodaJSONObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
