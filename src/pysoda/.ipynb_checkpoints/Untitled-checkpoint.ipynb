{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f94a06-7db4-406c-9081-b28104c8af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/envs/env-electron-python/lib/python3.6/site-packages/jose/backends/cryptography_backend.py:18: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes, int_to_bytes\n"
     ]
    }
   ],
   "source": [
    "### Import required python modules\n",
    "from gevent import monkey; monkey.patch_all()\n",
    "import platform\n",
    "import os\n",
    "from os import listdir, stat, makedirs, mkdir, walk, remove, pardir\n",
    "from os.path import isdir, isfile, join, splitext, getmtime, basename, normpath, exists, expanduser, split, dirname, getsize, abspath\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "import shutil\n",
    "from shutil import copy2\n",
    "from configparser import ConfigParser\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "from websocket import create_connection\n",
    "import socket\n",
    "import errno\n",
    "import re\n",
    "import gevent\n",
    "from pennsieve import Pennsieve\n",
    "from pennsieve.log import get_logger\n",
    "from pennsieve.api.agent import agent_cmd\n",
    "from pennsieve.api.agent import AgentError, check_port, socket_address\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import collections\n",
    "from threading import Thread\n",
    "import pathlib\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill, Font\n",
    "from docx import Document\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from pysoda import bf_get_current_user_permission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d12146e8-5c26-46e9-8aad-3271c2fc0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_get_dataset_files_folders(soda_json_structure, requested_sparc_only = True):\n",
    "    \"\"\"\n",
    "    Function for importing Pennsieve data files info into the \"dataset-structure\" key of the soda json structure,\n",
    "    including metadata from any existing manifest files in the high-level folders\n",
    "    (name, id, timestamp, description, additional metadata)\n",
    "\n",
    "    Args:\n",
    "        soda_json_structure: soda structure with bf account and dataset info available\n",
    "    Output:\n",
    "        same soda structure with Pennsieve data file info included under the \"dataset-structure\" key\n",
    "    \"\"\"\n",
    "\n",
    "    high_level_sparc_folders = [\"code\", \"derivative\", \"docs\", \"primary\", \"protocol\", \"source\"]\n",
    "    manifest_sparc = [\"manifest.xlsx\", \"manifest.csv\"]\n",
    "    high_level_metadata_sparc = ['submission.xlsx', 'submission.csv', 'submission.json', 'dataset_description.xlsx', \n",
    "                                 'dataset_description.csv', 'dataset_description.json', 'subjects.xlsx', 'subjects.csv', 'subjects.json', \n",
    "                                 'samples.xlsx', 'samples.csv', 'samples.json', 'README.txt', 'CHANGES.txt']\n",
    "    manifest_error_message = []\n",
    "    double_extensions = ['.ome.tiff','.ome.tif','.ome.tf2,','.ome.tf8','.ome.btf','.ome.xml','.brukertiff.gz','.mefd.gz','.moberg.gz',\n",
    "                         '.nii.gz','.mgh.gz','.tar.gz','.bcl.gz']\n",
    "\n",
    "    #f = open(\"dataset_contents.soda\", \"a\")\n",
    "\n",
    "    def verify_file_name(file_name, extension):\n",
    "        if extension == \"\":\n",
    "            return file_name\n",
    "        \n",
    "        double_ext = False\n",
    "        for ext in double_extensions:\n",
    "            if file_name.find(ext) != -1:\n",
    "                double_ext = True\n",
    "                break\n",
    "            \n",
    "        extension_from_name = \"\"\n",
    "\n",
    "        if double_ext == False:\n",
    "            extension_from_name = os.path.splitext(file_name)[1]\n",
    "        else:\n",
    "            extension_from_name = os.path.splitext(os.path.splitext(file_name)[0])[1] + os.path.splitext(file_name)[1]\n",
    "        \n",
    "        if extension_from_name == ('.' + extension):\n",
    "            return file_name\n",
    "        else:\n",
    "            return file_name + ('.' + extension)\n",
    "\n",
    "    # Add a new key containing the path to all the files and folders on the\n",
    "    # local data structure..\n",
    "    def recursive_item_path_create(folder, path):\n",
    "        if \"files\" in folder.keys():\n",
    "            for item in list(folder[\"files\"]):\n",
    "                if \"bfpath\" not in folder[\"files\"][item]:\n",
    "                    folder[\"files\"][item]['bfpath'] = path[:]\n",
    "\n",
    "        if \"folders\" in folder.keys():\n",
    "            for item in list(folder[\"folders\"]):\n",
    "                if \"bfpath\" not in folder[\"folders\"][item]:\n",
    "                    folder[\"folders\"][item]['bfpath'] = path[:]\n",
    "                    folder[\"folders\"][item]['bfpath'].append(item)\n",
    "                recursive_item_path_create(folder[\"folders\"][item], folder[\"folders\"][item]['bfpath'][:])\n",
    "        return\n",
    "\n",
    "    level = 0\n",
    "\n",
    "    def recursive_dataset_import(my_item, dataset_folder, metadata_files, my_folder_name, my_level, manifest_dict):\n",
    "        level = 0\n",
    "        col_count = 0\n",
    "        file_count = 0\n",
    "\n",
    "        for item in my_item:\n",
    "            if item.type == \"Collection\":\n",
    "                if \"folders\" not in dataset_folder:\n",
    "                    dataset_folder[\"folders\"] = {}\n",
    "                if \"files\" not in dataset_folder:\n",
    "                    dataset_folder[\"files\"] = {}\n",
    "                col_count += 1\n",
    "                folder_name = item.name\n",
    "# #                 if col_count == 1:\n",
    "#                     level = my_level + 1\n",
    "#                 print(my_level, folder_name not in high_level_sparc_folders, requested_sparc_only)\n",
    "                print(item.name, folder_name not in high_level_sparc_folders,  my_level, col_count)\n",
    "                if my_level == 0 and folder_name not in high_level_sparc_folders and requested_sparc_only:  # only import SPARC folders\n",
    "                    col_count -= 1\n",
    "                    continue\n",
    "                if col_count == 1:\n",
    "                    level = my_level + 1\n",
    "                dataset_folder[\"folders\"][folder_name] = {\n",
    "                    \"type\": \"bf\", \"action\": [\"existing\"], \"path\": item.id}\n",
    "                sub_folder = dataset_folder[\"folders\"][folder_name]\n",
    "                if \"folders\" not in sub_folder:\n",
    "                    sub_folder[\"folders\"] = {}\n",
    "                if \"files\" not in sub_folder:\n",
    "                    sub_folder[\"files\"] = {}\n",
    "                recursive_dataset_import(\n",
    "                    item, sub_folder, metadata_files, folder_name, level, manifest_dict)\n",
    "            else:\n",
    "                if \"folders\" not in dataset_folder:\n",
    "                    dataset_folder[\"folders\"] = {}\n",
    "                if \"files\" not in dataset_folder:\n",
    "                    dataset_folder[\"files\"] = {}\n",
    "                package_id = item.id\n",
    "                package_details = bf._api._get(\n",
    "                    '/packages/' + str(package_id))\n",
    "                if (\"extension\" not in package_details):\n",
    "                    file_name = verify_file_name(package_details[\"content\"][\"name\"], \"\")\n",
    "                else:\n",
    "                    file_name = verify_file_name(package_details[\"content\"][\"name\"], package_details[\"extension\"])\n",
    "\n",
    "                if my_level == 0 and file_name in high_level_metadata_sparc:\n",
    "                    metadata_files[file_name] = {\n",
    "                        \"type\": \"bf\", \"action\": [\"existing\"], \"path\": item.id}\n",
    "\n",
    "                else:\n",
    "                    file_count += 1\n",
    "                    if my_level == 1 and file_name in manifest_sparc:\n",
    "                        file_details = bf._api._get('/packages/' + str(package_id) + '/view')\n",
    "                        file_id = file_details[0][\"content\"][\"id\"]\n",
    "                        manifest_url = bf._api._get(\n",
    "                            '/packages/' + str(package_id) + '/files/' + str(file_id))\n",
    "                        df = \"\"\n",
    "                        try:\n",
    "                            if (file_name.lower() == 'manifest.xlsx'):\n",
    "                                df = pd.read_excel(manifest_url['url'], engine='openpyxl')\n",
    "                            else:\n",
    "                                df = pd.read_csv(manifest_url['url'])\n",
    "                            manifest_dict[my_folder_name] = df\n",
    "                        except Exception as e:\n",
    "                            manifest_error_message.append(package_details[\"parent\"][\"content\"][\"name\"])\n",
    "                            pass\n",
    "                    else:\n",
    "                        timestamp = (package_details[\"content\"][\"createdAt\"]\n",
    "                                     .replace('.', ',').replace('+00:00', 'Z'))\n",
    "                        dataset_folder[\"files\"][file_name] = {\n",
    "                            \"type\": \"bf\",\"action\": [\"existing\"], \"path\": item.id, \"timestamp\": timestamp}\n",
    "\n",
    "    def recursive_manifest_info_import(my_folder, my_relative_path, manifest_df):\n",
    "\n",
    "        if \"files\" in my_folder.keys():\n",
    "            for file_key, file in my_folder[\"files\"].items():\n",
    "                    filename = join(my_relative_path, file_key)\n",
    "                    colum_headers = manifest_df.columns.tolist()\n",
    "                    filename.replace(\"\\\\\",\"/\")\n",
    "\n",
    "                    if filename in list(manifest_df[\"filename\"].values):\n",
    "                        if \"description\" in colum_headers:\n",
    "                            mydescription = manifest_df[manifest_df['filename'] == filename][\"description\"].values[0]\n",
    "                            if mydescription:\n",
    "                                file[\"description\"] = mydescription\n",
    "                        if \"Additional Metadata\" in colum_headers:\n",
    "                            my_additional_medata = manifest_df[manifest_df['filename'] == filename][\"Additional Metadata\"].values[0]\n",
    "                            if mydescription:\n",
    "                                file[\"additional-metadata\"] = my_additional_medata\n",
    "                        if \"timestamp\" in colum_headers:\n",
    "                            my_timestamp = manifest_df[manifest_df['filename'] == filename][\"timestamp\"].values[0]\n",
    "                            if my_timestamp:\n",
    "                                file[\"timestamp\"] = my_timestamp\n",
    "\n",
    "        if \"folders\" in my_folder.keys():\n",
    "            for folder_key, folder in my_folder[\"folders\"].items():\n",
    "                relative_path = join(my_relative_path, folder_key)\n",
    "\n",
    "                recursive_manifest_info_import(folder, relative_path, manifest_df)\n",
    "\n",
    "    # START\n",
    "\n",
    "    error = []\n",
    "\n",
    "    # check that the Pennsieve account is valid\n",
    "    try:\n",
    "        bf_account_name = soda_json_structure[\"bf-account-selected\"][\"account-name\"]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        bf = Pennsieve(bf_account_name)\n",
    "    except Exception as e:\n",
    "        error.append('Error: Please select a valid Pennsieve account')\n",
    "        raise Exception(error)\n",
    "\n",
    "    # check that the Pennsieve dataset is valid\n",
    "    try:\n",
    "        bf_dataset_name = soda_json_structure[\"bf-dataset-selected\"][\"dataset-name\"]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    try:\n",
    "        myds = bf.get_dataset(bf_dataset_name)\n",
    "    except Exception as e:\n",
    "        error.append('Error: Please select a valid Pennsieve dataset')\n",
    "        raise Exception(error)\n",
    "\n",
    "    # check that the user has permission to edit this dataset\n",
    "    try:\n",
    "        role = bf_get_current_user_permission(bf, myds)\n",
    "        if role not in ['owner', 'manager', 'editor']:\n",
    "            curatestatus = 'Done'\n",
    "            error.append(\"Error: You don't have permissions for uploading to this Pennsieve dataset\")\n",
    "            raise Exception(error)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        # import files and folders in the soda json structure\n",
    "        soda_json_structure[\"dataset-structure\"] = {}\n",
    "        soda_json_structure[\"metadata-files\"] = {}\n",
    "        dataset_folder = soda_json_structure[\"dataset-structure\"]\n",
    "        metadata_files = soda_json_structure[\"metadata-files\"]\n",
    "        manifest_dict = {}\n",
    "        folder_name = \"\"\n",
    "        recursive_dataset_import(myds, dataset_folder, metadata_files, folder_name, level, manifest_dict)\n",
    "\n",
    "        #remove metadata files keys if empty\n",
    "        metadata_files = soda_json_structure[\"metadata-files\"]\n",
    "        if not metadata_files:\n",
    "            del soda_json_structure['metadata-files']\n",
    "\n",
    "        dataset_folder = soda_json_structure[\"dataset-structure\"]\n",
    "        # pull information from the manifest files if they satisfy the SPARC format\n",
    "        if \"folders\" in dataset_folder.keys():\n",
    "            for folder_key in manifest_dict.keys():\n",
    "                manifest_df = manifest_dict[folder_key]\n",
    "                manifest_df = manifest_df.fillna('')\n",
    "                colum_headers = manifest_df.columns.tolist()\n",
    "                folder = dataset_folder[\"folders\"][folder_key]\n",
    "                if \"filename\" in colum_headers:\n",
    "                    if \"description\" in colum_headers or \"Additional Metadata\" in colum_headers:\n",
    "                        relative_path = \"\"\n",
    "                        recursive_manifest_info_import(folder, relative_path, manifest_df)\n",
    "\n",
    "        recursive_item_path_create(soda_json_structure[\"dataset-structure\"], [])\n",
    "        success_message = \"Data files under a valid high-level SPARC folders have been imported\"\n",
    "        return [soda_json_structure, success_message, manifest_error_message]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af3dc229-6c76-48bf-a572-0fa5fa0b797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc True 0 1\n",
      "derivative False 0 1\n",
      "subj-1 True 1 1\n",
      "sam-1 True 2 1\n",
      "sub-3 True 1 2\n",
      "sam-3 True 2 1\n",
      "sub-2 True 1 3\n",
      "sam-2 True 2 1\n",
      "primary False 0 2\n",
      "sub-3 True 1 1\n",
      "sam-3 True 2 1\n",
      "sub-2 True 1 2\n",
      "sam-2 True 2 1\n",
      "sub-1 True 1 3\n",
      "sam-1 True 2 1\n"
     ]
    }
   ],
   "source": [
    "soda_json_obj = {\n",
    "    \"bf-account-selected\": {\n",
    "        \"account-name\": \"SODA-Pennsieve\"\n",
    "    },\n",
    "    \"bf-dataset-selected\": {\n",
    "        \"dataset-name\": \"Test case for physiological data visualisation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "res = bf_get_dataset_files_folders(soda_json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a27ea66-98fb-4af6-9660-132da9b88472",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-41-0431aba34145>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-0431aba34145>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    doc True 0 1\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataset_description.xlsx\n",
    "doc\n",
    "doc True 0 1\n",
    "derivative\n",
    "derivative False 0 2\n",
    "subj-1\n",
    "subj-1 True 0 1\n",
    "manifest.xlsx\n",
    "sub-3\n",
    "sub-3 True 0 2\n",
    "sub-2\n",
    "sub-2 True 0 3\n",
    "primary\n",
    "primary False 0 3\n",
    "manifest.xlsx\n",
    "sub-3\n",
    "sub-3 True 0 1\n",
    "sub-2\n",
    "sub-2 True 0 2\n",
    "sub-1\n",
    "sub-1 True 0 3\n",
    "samples.xlsx\n",
    "subjects.xlsx\n",
    "submission.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33bf31b-c4d3-433b-a1d8-5df39835816d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
