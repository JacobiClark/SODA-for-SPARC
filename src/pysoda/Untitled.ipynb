{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required librairies\n",
    "import os\n",
    "from os import listdir, stat, makedirs, mkdir, walk, remove, pardir, rename\n",
    "from os.path import isdir, isfile, join, splitext, getmtime, basename, normpath, exists, expanduser, split, dirname, getsize, abspath\n",
    "from blackfynn import Blackfynn\n",
    "import json \n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import shutil\n",
    "import gevent\n",
    "import urllib.request\n",
    "import copy\n",
    "import re\n",
    "import platform\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import subprocess\n",
    "bf_recognized_file_extensions = ['.cram','.jp2','.jpx','.lsm','.ndpi','.nifti','.oib','.oif','.roi','.rtf','.swc','.abf','.acq','.adicht','.adidat',\\\n",
    "                                 '.aedt','.afni','.ai','.avi','.bam','.bash','.bcl','.bcl.gz','.bin','.brik','.brukertiff.gz','.continuous','.cpp','.csv',\\\n",
    "                                 '.curv','.cxls','.czi','.data','.dcm','.df','.dicom','.doc','.docx','.e','.edf','.eps','.events','.fasta','.fastq','.fcs',\\\n",
    "                                 '.feather','.fig','.gif','.h4','.h5','.hdf4','.hdf5','.hdr','.he2','.he5','.head','.hoc','.htm','.html','.ibw','.img','.ims',\\\n",
    "                                 '.ipynb','.jpeg','.jpg','.js','.json','.lay','.lh','.lif','.m','.mat','.md','.mef','.mefd.gz','.mex','.mgf','.mgh','.mgh.gz',\\\n",
    "                                 '.mgz','.mnc','.moberg.gz','.mod','.mov','.mp4','.mph','.mpj','.mtw','.ncs','.nd2','.nev','.nex','.nex5','.nf3','.nii','.nii.gz',\\\n",
    "                                 '.ns1','.ns2','.ns3','.ns4','.ns5','.ns6','.nwb','.ogg','.ogv','.ome.btf','.ome.tif','.ome.tif2','.ome.tif8','.ome.tiff','.ome.xml',\\\n",
    "                                 '.openephys','.pdf','.pgf','.png','.ppt','.pptx','.ps','.pul','.py','.r','.raw','.rdata','.rh','.rhd','.sh','.sldasm','.slddrw',\\\n",
    "                                 '.smr','.spikes','.svg','.svs','.tab','.tar','.tar.gz','.tcsh','.tdm','.tdms','.text','.tif','.tiff','.tsv','.txt','.vcf','.webm',\\\n",
    "                                 '.xlsx','.xml','.yaml','.yml','.zip','.zsh']\n",
    "# Define constants\n",
    "userpath = expanduser(\"~\")\n",
    "\n",
    "### Internal functions\n",
    "def TZLOCAL():\n",
    "    return datetime.now(timezone.utc).astimezone().tzinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_get_current_user_permission(bf, myds):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to get the permission of currently logged in user for a selected dataset\n",
    "\n",
    "    Args:\n",
    "        bf: logged Blackfynn acccount (dict)\n",
    "        myds: selected Blackfynn dataset (dict)\n",
    "    Output:\n",
    "        permission of current user (string)\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        selected_dataset_id = myds.id\n",
    "        user_role = bf._api._get('/datasets/' + str(selected_dataset_id) + '/role')['role']\n",
    "\n",
    "        return user_role\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_high_level_manifest_files_existing_bf(soda_json_structure, bf, ds):\n",
    "    \"\"\"\n",
    "    Function to create manifest files for each high-level SPARC folder.\n",
    "\n",
    "    Args:\n",
    "        soda_json_structure: soda dict with information about the dataset to be generated/modified\n",
    "    Action:\n",
    "        manifest_files_structure: dict including the local path of the manifest files\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        def recursive_manifest_info_import_bf(my_item, my_relative_path, dict_folder_manifest, manifest_df):\n",
    "\n",
    "            for item in my_item.items:\n",
    "                if item.type == \"Collection\":\n",
    "                    folder_name = item.name\n",
    "                    relative_path = generate_relative_path(my_relative_path, folder_name)\n",
    "                    dict_folder_manifest = recursive_manifest_info_import_bf(item, relative_path, dict_folder_manifest, manifest_df)\n",
    "                else:\n",
    "                    if item.name != 'manifest':\n",
    "                        file_id = item.id\n",
    "                        file_details = bf._api._get('/packages/' + str(file_id) + '/view')\n",
    "                        file_name = file_details[0][\"content\"][\"name\"]\n",
    "                        file_extension = splitext(file_name)[1]\n",
    "                        file_name_with_extension = splitext(item.name)[0] + file_extension\n",
    "                        relative_path = generate_relative_path(my_relative_path, file_name_with_extension)\n",
    "                        dict_folder_manifest[\"filename\"].append(relative_path)\n",
    "\n",
    "                        #file type\n",
    "                        file_extension = splitext(file_name)[1]\n",
    "                        dict_folder_manifest[\"file type\"].append(file_extension)\n",
    "\n",
    "                        #timestamp, description, Additional Metadata\n",
    "                        if not manifest_df.empty:\n",
    "                            if relative_path in manifest_df[\"filename\"].values:\n",
    "                                timestamp = manifest_df[manifest_df[\"filename\"] == relative_path][\"timestamp\"].iloc[0]\n",
    "                                description = manifest_df[manifest_df[\"filename\"] == relative_path][\"description\"].iloc[0]\n",
    "                                additional_metadata = manifest_df[manifest_df[\"filename\"] == relative_path][\"Additional Metadata\"].iloc[0]\n",
    "                            else:\n",
    "                                timestamp = \"\"\n",
    "                                description = \"\"\n",
    "                                additional_metadata= \"\"\n",
    "                            dict_folder_manifest[\"timestamp\"].append(timestamp)\n",
    "                            dict_folder_manifest[\"description\"].append(description)\n",
    "                            dict_folder_manifest[\"Additional Metadata\"].append(additional_metadata)\n",
    "                        else:\n",
    "                            dict_folder_manifest[\"timestamp\"].append(\"\")\n",
    "                            dict_folder_manifest[\"description\"].append(\"\")\n",
    "                            dict_folder_manifest[\"Additional Metadata\"].append(\"\")\n",
    "\n",
    "            return dict_folder_manifest\n",
    "\n",
    "\n",
    "        # Merge existing folders\n",
    "        def recursive_manifest_builder_existing_bf(my_folder, my_bf_folder, my_bf_folder_exists, my_relative_path, dict_folder_manifest):\n",
    "\n",
    "            if \"folders\" in my_folder.keys():\n",
    "                if my_bf_folder_exists:\n",
    "                    my_bf_existing_folders, my_bf_existing_folders_name = bf_get_existing_folders_details(my_bf_folder)\n",
    "                else:\n",
    "                    my_bf_existing_folders = []\n",
    "                    my_bf_existing_folders_name = []\n",
    "\n",
    "                for folder_key, folder in my_folder[\"folders\"].items():\n",
    "                    relative_path = generate_relative_path(my_relative_path, folder_key)\n",
    "                    if folder_key in my_bf_existing_folders_name:\n",
    "                        bf_folder_index = my_bf_existing_folders_name.index(folder_key)\n",
    "                        bf_folder = my_bf_existing_folders[bf_folder_index]\n",
    "                        bf_folder_exists = True\n",
    "                    else:\n",
    "                        bf_folder = ''\n",
    "                        bf_folder_exists = False\n",
    "                    dict_folder_manifest = recursive_manifest_builder_existing_bf(folder, bf_folder, bf_folder_exists, relative_path, dict_folder_manifest)\n",
    "\n",
    "            if \"files\" in my_folder.keys():\n",
    "                if my_bf_folder_exists:\n",
    "                    my_bf_existing_files, my_bf_existing_files_name, my_bf_existing_files_name_with_extension = bf_get_existing_files_details(my_bf_folder)\n",
    "                else:\n",
    "                    my_bf_existing_files = []\n",
    "                    my_bf_existing_files_name = []\n",
    "                    my_bf_existing_files_name_with_extension = []\n",
    "\n",
    "                for file_key, file in my_folder[\"files\"].items():\n",
    "                    gevent.sleep(0)\n",
    "                    if file[\"type\"] == \"local\":\n",
    "                        file_path = file[\"path\"]\n",
    "                        if isfile(file_path):\n",
    "                            desired_name = splitext(file_key)[0]\n",
    "                            file_extension = splitext(file_key)[1]\n",
    "\n",
    "                            # manage existing file request\n",
    "                            if existing_file_option == \"skip\":\n",
    "                                if file_key in my_bf_existing_files_name_with_extension:\n",
    "                                    continue\n",
    "\n",
    "                            if existing_file_option == \"replace\":\n",
    "                                if file_key in my_bf_existing_files_name_with_extension:\n",
    "                                    #remove existing from manifest\n",
    "                                    filename = generate_relative_path(my_relative_path, file_key)\n",
    "                                    filename_list = dict_folder_manifest[\"filename\"]\n",
    "                                    index_file = filename_list.index(filename)\n",
    "                                    del dict_folder_manifest[\"filename\"][index_file]\n",
    "                                    del dict_folder_manifest[\"timestamp\"][index_file]\n",
    "                                    del dict_folder_manifest[\"description\"][index_file]\n",
    "                                    del dict_folder_manifest[\"file type\"][index_file]\n",
    "                                    del dict_folder_manifest[\"Additional Metadata\"][index_file]\n",
    "\n",
    "                                    index_name = my_bf_existing_files_name_with_extension.index(file_key)\n",
    "                                    del my_bf_existing_files[index_name]\n",
    "                                    del my_bf_existing_files_name[index_name]\n",
    "                                    del my_bf_existing_files_name_with_extension[index_name]\n",
    "\n",
    "                            if desired_name not in my_bf_existing_files_name:\n",
    "                                final_name = file_key\n",
    "                            else:\n",
    "\n",
    "                                # expected final name\n",
    "                                count_done = 0\n",
    "                                final_name = desired_name\n",
    "                                output = get_base_file_name(desired_name)\n",
    "                                if output:\n",
    "                                    base_name = output[0]\n",
    "                                    count_exist = output[1]\n",
    "                                    while count_done == 0:\n",
    "                                        if final_name in my_bf_existing_files_name:\n",
    "                                            count_exist += 1\n",
    "                                            final_name = base_name + \"(\" + str(count_exist) + \")\"\n",
    "                                        else:\n",
    "                                            count_done = 1\n",
    "                                else:\n",
    "                                    count_exist = 0\n",
    "                                    while count_done == 0:\n",
    "                                        if final_name in my_bf_existing_files_name:\n",
    "                                            count_exist += 1\n",
    "                                            final_name = desired_name + \" (\" + str(count_exist) + \")\"\n",
    "                                        else:\n",
    "                                            count_done = 1\n",
    "\n",
    "                                final_name = final_name + file_extension\n",
    "                                my_bf_existing_files_name.append(splitext(final_name)[0])\n",
    "\n",
    "                            #filename\n",
    "                            filename = generate_relative_path(my_relative_path, final_name)\n",
    "                            dict_folder_manifest[\"filename\"].append(filename)\n",
    "\n",
    "                            #timestamp\n",
    "                            file_path = file[\"path\"]\n",
    "                            filepath = pathlib.Path(file_path)\n",
    "                            mtime = filepath.stat().st_mtime\n",
    "                            lastmodtime = datetime.fromtimestamp(mtime).astimezone(local_timezone)\n",
    "                            dict_folder_manifest[\"timestamp\"].append(lastmodtime.isoformat().replace('.', ',').replace('+00:00', 'Z'))\n",
    "\n",
    "                            #description\n",
    "                            if \"description\" in file.keys():\n",
    "                                dict_folder_manifest[\"description\"].append(file[\"description\"])\n",
    "                            else:\n",
    "                                dict_folder_manifest[\"description\"].append(\"\")\n",
    "\n",
    "                            #file type\n",
    "                            if file_extension == \"\":\n",
    "                                file_extension = \"None\"\n",
    "                            dict_folder_manifest[\"file type\"].append(file_extension)\n",
    "\n",
    "                            #addtional metadata\n",
    "                            if \"additional-metadata\" in file.keys():\n",
    "                                dict_folder_manifest[\"Additional Metadata\"].append(file[\"additional-metadata\"])\n",
    "                            else:\n",
    "                                dict_folder_manifest[\"Additional Metadata\"].append(\"\")\n",
    "\n",
    "            return dict_folder_manifest\n",
    "\n",
    "        #create local folder to save manifest files temporarly (delete any existing one first)\n",
    "        shutil.rmtree(manifest_folder_path) if isdir(manifest_folder_path) else 0\n",
    "        makedirs(manifest_folder_path)\n",
    "\n",
    "        # import info about files already on bf\n",
    "        dataset_structure = soda_json_structure[\"dataset-structure\"]\n",
    "        manifest_dict_save = {}\n",
    "        for item in ds.items:\n",
    "            if item.type == \"Collection\" and item.name in dataset_structure[\"folders\"].keys():\n",
    "\n",
    "                    relative_path = \"\"\n",
    "                    item_id = item.id\n",
    "                    # Initialize dict where manifest info will be stored\n",
    "                    dict_folder_manifest = {}\n",
    "                    dict_folder_manifest[\"filename\"] = []\n",
    "                    dict_folder_manifest[\"timestamp\"] = []\n",
    "                    dict_folder_manifest[\"description\"] = []\n",
    "                    dict_folder_manifest[\"file type\"] = []\n",
    "                    dict_folder_manifest[\"Additional Metadata\"] = []\n",
    "\n",
    "                    # pull manifest file into if exists\n",
    "                    manifest_df = pd.DataFrame()\n",
    "                    for file in item.items:\n",
    "                        if file.type != \"Collection\":\n",
    "                            file_id = file.id\n",
    "                            file_details = bf._api._get('/packages/' + str(file_id) +'/view')\n",
    "                            file_name_with_extension = file_details[0]['content']['name']\n",
    "                            if file_name_with_extension in manifest_sparc:\n",
    "                                file_id_2 = file_details[0][\"content\"][\"id\"]\n",
    "                                file_url_info = bf._api._get('/packages/' + str(file_id) + '/files/' + str(file_id_2))\n",
    "                                file_url = file_url_info['url']\n",
    "                                manifest_df = pd.read_excel(file_url)\n",
    "                                manifest_df = manifest_df.fillna('')\n",
    "                                if \"filename\" not in manifest_df.columns or \"description\" not in manifest_df.columns or \"Additional Metadata\" not in manifest_df.columns:\n",
    "                                    manifest_df = pd.DataFrame()\n",
    "                                break\n",
    "                    dict_folder_manifest = recursive_manifest_info_import_bf(item, relative_path, dict_folder_manifest, manifest_df)\n",
    "                    manifest_dict_save[item.name] = {'manifest': dict_folder_manifest, 'bf_folder': item}\n",
    "\n",
    "\n",
    "        # import info from local files to be uploaded\n",
    "        local_timezone = TZLOCAL()\n",
    "        manifest_files_structure = {}\n",
    "        existing_folder_option = soda_json_structure[\"generate-dataset\"][\"if-existing\"]\n",
    "        existing_file_option = soda_json_structure[\"generate-dataset\"][\"if-existing-files\"]\n",
    "        for folder_key, folder in dataset_structure[\"folders\"].items():\n",
    "            relative_path = ''\n",
    "\n",
    "            if folder_key in manifest_dict_save.keys() and existing_folder_option == \"merge\":\n",
    "                bf_folder = manifest_dict_save[folder_key]['bf_folder']\n",
    "                bf_folder_exists = True\n",
    "                dict_folder_manifest = manifest_dict_save[folder_key]['manifest']\n",
    "\n",
    "            elif folder_key in manifest_dict_save.keys() and existing_folder_option == \"skip\":\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                bf_folder = ''\n",
    "                bf_folder_exists = False\n",
    "                dict_folder_manifest = {}\n",
    "                dict_folder_manifest[\"filename\"] = []\n",
    "                dict_folder_manifest[\"timestamp\"] = []\n",
    "                dict_folder_manifest[\"description\"] = []\n",
    "                dict_folder_manifest[\"file type\"] = []\n",
    "                dict_folder_manifest[\"Additional Metadata\"] = []\n",
    "\n",
    "            dict_folder_manifest = recursive_manifest_builder_existing_bf(folder, bf_folder, bf_folder_exists, relative_path, dict_folder_manifest)\n",
    "\n",
    "            #create high-level folder at the temporary location\n",
    "            folderpath = join(manifest_folder_path, folder_key)\n",
    "            makedirs(folderpath)\n",
    "\n",
    "            #save manifest file\n",
    "            manifestfilepath = join(folderpath, 'manifest.xlsx')\n",
    "            df = pd.DataFrame.from_dict(dict_folder_manifest)\n",
    "            df.to_excel(manifestfilepath, index=None, header=True)\n",
    "\n",
    "            manifest_files_structure[folder_key] = manifestfilepath\n",
    "\n",
    "        return manifest_files_structure\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_get_dataset_files_folders(soda_json_structure, requested_sparc_only = True):\n",
    "    \"\"\"\n",
    "    Function for importing blackfynn data files info into the \"dataset-structure\" key of the soda json structure, \n",
    "    including metadata from any existing manifest files in the high-level folders \n",
    "    (name, id, timestamp, description, additional metadata)\n",
    "\n",
    "    Args:\n",
    "        soda_json_structure: soda structure with bf account and dataset info available\n",
    "    Output:\n",
    "        same soda structure with blackfyn data file info included under the \"dataset-structure\" key\n",
    "    \"\"\"\n",
    "\n",
    "    high_level_sparc_folders = [\"code\", \"derivative\", \"docs\", \"primary\", \"protocol\", \"source\"]\n",
    "    manifest_sparc = [\"manifest.xlsx\", \"manifest.csv\"]\n",
    "    high_level_metadata_sparc = ['submission.xlsx', 'submission.csv', 'submission.json', 'dataset_description.xlsx', 'dataset_description.csv', 'dataset_description.json', 'subjects.xlsx', 'subjects.csv', 'subjects.json', 'samples.xlsx', 'samples.csv', 'samples.json', 'README.txt', 'CHANGES.txt']\n",
    "\n",
    "    def verify_file_name(item_name, file_name):\n",
    "        filename, file_extension = os.path.splitext(file_name)\n",
    "        if file_extension in bf_recognized_file_extensions:\n",
    "            return item_name + file_extension\n",
    "        else:\n",
    "            return file_name\n",
    "\n",
    "    def recursive_dataset_import(my_item, dataset_folder, metadata_files, my_folder_name, my_level, manifest_dict):\n",
    "        col_count = 0\n",
    "        file_count = 0\n",
    "\n",
    "        for item in my_item:\n",
    "            if item.type == \"Collection\":\n",
    "                if \"folders\" not in dataset_folder:\n",
    "                    dataset_folder[\"folders\"] = {}\n",
    "                if \"files\" not in dataset_folder:\n",
    "                    dataset_folder[\"files\"] = {}\n",
    "                col_count += 1\n",
    "                folder_name = item.name\n",
    "                if my_level == 0 and folder_name not in high_level_sparc_folders and requested_sparc_only:  # only import SPARC folders\n",
    "                    continue\n",
    "                if col_count == 1:\n",
    "                    #dataset_folder[\"folders\"] = {}\n",
    "                    level = my_level + 1\n",
    "                dataset_folder[\"folders\"][folder_name] = {\n",
    "                    \"type\": \"bf\", \"action\": [\"existing\"], \"path\": item.id}\n",
    "                sub_folder = dataset_folder[\"folders\"][folder_name]\n",
    "                if \"folders\" not in sub_folder:\n",
    "                    sub_folder[\"folders\"] = {}\n",
    "                if \"files\" not in sub_folder:\n",
    "                    sub_folder[\"files\"] = {}\n",
    "                recursive_dataset_import(\n",
    "                    item, sub_folder, metadata_files, folder_name, level, manifest_dict)\n",
    "            else:\n",
    "                if \"folders\" not in dataset_folder:\n",
    "                    dataset_folder[\"folders\"] = {}\n",
    "                if \"files\" not in dataset_folder:\n",
    "                    dataset_folder[\"files\"] = {}\n",
    "                package_id = item.id\n",
    "                file_details = bf._api._get(\n",
    "                    '/packages/' + str(package_id) + '/view')\n",
    "                file_name = file_details[0][\"content\"][\"name\"]\n",
    "                file_name = verify_file_name(item.name, file_name)\n",
    "\n",
    "                if my_level == 0 and file_name in high_level_metadata_sparc:\n",
    "                    metadata_files[file_name] = {\n",
    "                        \"type\": \"bf\", \"action\": [\"existing\"], \"path\": item.id}\n",
    "\n",
    "                else:\n",
    "                    file_count += 1\n",
    "                    #if file_count == 1:\n",
    "                    #dataset_folder[\"files\"] = {}\n",
    "#                     if my_level == 0:\n",
    "#                         dataset_folder[\"files\"][file_name] = {\"type\": \"bf\", \"action\": [\"existing\"], \"path\": item.id}\n",
    "                    if my_level == 1 and file_name in manifest_sparc:\n",
    "                        file_id = file_details[0][\"content\"][\"id\"]\n",
    "                        manifest_url = bf._api._get(\n",
    "                            '/packages/' + str(package_id) + '/files/' + str(file_id))\n",
    "                        df = pd.read_excel(manifest_url['url'])\n",
    "                        manifest_dict[my_folder_name] = df\n",
    "                    else:\n",
    "                        timestamp = file_details[0][\"content\"][\"updatedAt\"]\n",
    "                        dataset_folder[\"files\"][file_name] = {\n",
    "                            \"type\": \"bf\",\"action\": [\"existing\"], \"path\": item.id, \"timestamp\": timestamp}\n",
    "\n",
    "\n",
    "    def recursive_manifest_info_import(my_folder, my_relative_path, manifest_df):\n",
    "        \n",
    "        if \"files\" in my_folder.keys():\n",
    "            for file_key, file in my_folder[\"files\"].items():\n",
    "                    filename = join(my_relative_path, file_key)\n",
    "                    colum_headers = manifest_df.columns.tolist()\n",
    "                    if filename in list(manifest_df[\"filename\"].values):\n",
    "                        if \"description\" in colum_headers:\n",
    "                            mydescription = manifest_df[manifest_df['filename'] == filename][\"description\"].values[0]\n",
    "                            if mydescription:\n",
    "                                file[\"description\"] = mydescription\n",
    "                        if \"Additional Metadata\" in colum_headers:\n",
    "                            my_additional_medata = manifest_df[manifest_df['filename'] == filename][\"Additional Metadata\"].values[0]\n",
    "                            if mydescription:\n",
    "                                file[\"additional-metadata\"] = my_additional_medata\n",
    "\n",
    "        if \"folders\" in my_folder.keys():\n",
    "            for folder_key, folder in my_folder[\"folders\"].items():\n",
    "                relative_path = join(my_relative_path, folder_key)\n",
    "                recursive_manifest_info_import(folder, relative_path, manifest_df)\n",
    "    \n",
    "    # START\n",
    "    \n",
    "    error = []\n",
    "    \n",
    "    # check that the blackfynn account is valid\n",
    "    try:\n",
    "        bf_account_name = soda_json_structure[\"bf-account-selected\"][\"account-name\"]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        \n",
    "    try:\n",
    "        bf = Blackfynn(bf_account_name)\n",
    "    except Exception as e:\n",
    "        error.append('Error: Please select a valid Blackfynn account')\n",
    "        raise Exception(error)  \n",
    "\n",
    "    # check that the blackfynn dataset is valid\n",
    "    try:\n",
    "        bf_dataset_name = soda_json_structure[\"bf-dataset-selected\"][\"dataset-name\"]\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    try:\n",
    "        myds = bf.get_dataset(bf_dataset_name)\n",
    "    except Exception as e:\n",
    "        error.append('Error: Please select a valid Blackfynn dataset')\n",
    "        raise Exception(error)\n",
    "    \n",
    "    # check that the user has permission to edit this dataset\n",
    "    try:\n",
    "        role = bf_get_current_user_permission(bf, myds)\n",
    "        if role not in ['owner', 'manager', 'editor']:\n",
    "            curatestatus = 'Done'\n",
    "            error.append(\"Error: You don't have permissions for uploading to this Blackfynn dataset\")\n",
    "            raise Exception(error)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "    try:\n",
    "        # import files and folders in the soda json structure\n",
    "        soda_json_structure[\"dataset-structure\"] = {}\n",
    "        soda_json_structure[\"metadata-files\"] = {}\n",
    "        dataset_folder = soda_json_structure[\"dataset-structure\"]\n",
    "        metadata_files = soda_json_structure[\"metadata-files\"]\n",
    "        manifest_dict = {}\n",
    "        level = 0\n",
    "        folder_name = \"\"\n",
    "        recursive_dataset_import(myds, dataset_folder, metadata_files, folder_name, level, manifest_dict)\n",
    "        \n",
    "        #remove metadata files keys if empty\n",
    "        metadata_files = soda_json_structure[\"metadata-files\"]\n",
    "        if not metadata_files:\n",
    "            del soda_json_structure['metadata-files']\n",
    "        \n",
    "        # pull information from the manifest files if they satisfy the SPARC format\n",
    "        if \"folders\" in dataset_folder.keys():\n",
    "            for folder_key in manifest_dict.keys():\n",
    "                manifest_df = manifest_dict[folder_key]\n",
    "                manifest_df = manifest_df.fillna('')  \n",
    "                colum_headers = manifest_df.columns.tolist()\n",
    "                folder = dataset_folder[\"folders\"][folder_key]\n",
    "                if \"filename\" in colum_headers:\n",
    "                    if \"description\" in colum_headers or \"Additional Metadata\" in colum_headers:\n",
    "                        relative_path = \"\"\n",
    "                        recursive_manifest_info_import(folder, relative_path, manifest_df)\n",
    "\n",
    "        success_message = \"Data files under a valid high-level SPARC folders have been imported\"\n",
    "        return [soda_json_structure, success_message]\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_update_existing_dataset(soda_json_structure, bf, ds):    \n",
    "    global main_curate_progress_message\n",
    "    global main_total_generate_dataset_size\n",
    "    global start_generate\n",
    "    global main_initial_bfdataset_size\n",
    "    bfsd = \"\"\n",
    "\n",
    "    # Delete any files on blackfynn that have been marked as deleted\n",
    "    def recursive_file_delete(folder):\n",
    "        if \"files\" in folder.keys():\n",
    "            for item in list(folder[\"files\"]):\n",
    "                if \"deleted\" in folder[\"files\"][item]['action']:\n",
    "                    file = bf.get(folder[\"files\"][item]['path'])\n",
    "                    file.delete()\n",
    "                    del folder[\"files\"][item]\n",
    "\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            recursive_file_delete(folder[\"folders\"][item])\n",
    "        return\n",
    "    \n",
    "    # Add a new key containing the path to all the files and folders on the \n",
    "    # local data structure.\n",
    "    # Allows us to see if the folder path of a specfic file already \n",
    "    # exists on blackfynn.\n",
    "    def recursive_item_path_create(folder, path):\n",
    "        if \"files\" in folder.keys():\n",
    "            for item in list(folder[\"files\"]):\n",
    "                if \"folderpath\" not in folder[\"files\"][item]:\n",
    "                    folder[\"files\"][item]['folderpath'] = path[:]\n",
    "\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            if \"folderpath\" not in folder[\"folders\"][item]:\n",
    "                folder[\"folders\"][item]['folderpath'] = path[:]\n",
    "                folder[\"folders\"][item]['folderpath'].append(item)\n",
    "            recursive_item_path_create(folder[\"folders\"][item], folder[\"folders\"][item]['folderpath'][:])\n",
    "\n",
    "        return\n",
    "\n",
    "    # Check and create any non existing folders for the file move process\n",
    "    def recursive_check_and_create_bf_file_path(folderpath, index, bfsd):\n",
    "        folder = folderpath[index]\n",
    "        \n",
    "        if folder not in bfsd[\"folders\"]:\n",
    "            if (index == 0):\n",
    "                new_folder = ds.create_collection(folder)\n",
    "            else:\n",
    "                current_folder = bf.get(bfsd[\"path\"])\n",
    "                new_folder = current_folder.create_collection(folder)\n",
    "            bfsd[\"folders\"][folder] = {\"type\": \"bf\", \"action\": [\"existing\"], \"path\": new_folder.id, \"folders\":{}, \"files\":{}}\n",
    "            \n",
    "        index += 1\n",
    "        \n",
    "        if index < len(folderpath):\n",
    "            recursive_check_and_create_bf_file_path(folderpath, index, bfsd[\"folders\"][folder])\n",
    "        else:\n",
    "            return bfsd[\"folders\"][folder][\"path\"]\n",
    "\n",
    "    # Check for any files that have been moved and verify paths before moving\n",
    "    def recursive_check_moved_files(folder):\n",
    "        if \"files\" in folder.keys():\n",
    "            for item in list(folder[\"files\"]):\n",
    "                if \"moved\" in folder[\"files\"][item]['action'] and folder[\"files\"][item][\"type\"] == \"bf\":\n",
    "                    new_folder_id = \"\"\n",
    "                    new_folder_id = recursive_check_and_create_bf_file_path(folder[\"files\"][item][\"folderpath\"].copy(), 0, bfsd)\n",
    "                    destination_folder = bf.get(new_folder_id)\n",
    "                    bf.move(destination_folder, folder[\"files\"][item][\"path\"])\n",
    "\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            recursive_check_moved_files(folder[\"folders\"][item])\n",
    "\n",
    "        return\n",
    "\n",
    "    # Rename any files that exist on blackfynn\n",
    "    def recursive_file_rename(folder):\n",
    "        if \"files\" in folder.keys():\n",
    "            for item in list(folder[\"files\"]):\n",
    "                if \"renamed\" in folder[\"files\"][item]['action'] and folder[\"files\"][item][\"type\"] == \"bf\":\n",
    "                    file = bf.get(folder[\"files\"][item][\"path\"])\n",
    "                    file.name = item\n",
    "                    file.update()\n",
    "\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            recursive_file_rename(folder[\"folders\"][item])\n",
    "\n",
    "        return\n",
    "    \n",
    "    # Delete any stray folders that exist on blackfynn\n",
    "    # Only top level files are deleted since the api deletes any \n",
    "    # files and folders that exist inside.\n",
    "    def recursive_folder_delete(folder):\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            if \"deleted\" in folder[\"folders\"][item]['action']:\n",
    "                file = bf.get(folder[\"folders\"][item]['path'])\n",
    "                file.delete()\n",
    "                del folder[\"folders\"][item]\n",
    "            else:\n",
    "                recursive_folder_delete(folder[\"folders\"][item])\n",
    "\n",
    "        return\n",
    "\n",
    "    # Rename any folders that still exist.\n",
    "    def recursive_folder_rename(folder):\n",
    "        for item in list(folder[\"folders\"]):\n",
    "            if \"renamed\" in folder[\"folders\"][item]['action'] and folder[\"folders\"][item][\"type\"] == \"bf\":\n",
    "                file = bf.get(folder[\"folders\"][item][\"path\"])\n",
    "                file.name = item\n",
    "                file.update()\n",
    "        else:\n",
    "            recursive_file_rename(folder[\"folders\"][item])\n",
    "\n",
    "        return\n",
    "\n",
    "    # 1. Remove all existing files on blackfynn, that the user deleted.\n",
    "    main_curate_progress_message = \"Deleting files on blackfynn\"\n",
    "    dataset_structure = soda_json_structure[\"dataset-structure\"]\n",
    "    recursive_file_delete(dataset_structure)\n",
    "    main_curate_progress_message = \"Files on blackfynn marked for deletion have been deleted\"\n",
    "    \n",
    "    # 2. Get the status of all files currently on blackfynn and create \n",
    "    # the folderpath for all items in both dataset structures.\n",
    "    main_curate_progress_message = \"Retreiving files and folders from blackfynn\"\n",
    "    current_bf_dataset_files_folders = bf_get_dataset_files_folders (soda_json_structure)[0]\n",
    "    bfsd = current_bf_dataset_files_folders[\"dataset-structure\"]\n",
    "    main_curate_progress_message = \"Creating file paths for all files on blackfynn\"\n",
    "    recursive_item_path_create(dataset_structure, [])\n",
    "    recursive_item_path_create(bfsd, [])\n",
    "    main_curate_progress_message = \"File paths created\"\n",
    "    \n",
    "\n",
    "    # 3. Move any files that are marked as moved on blackfynn. \n",
    "    # Create any additional folders if required\n",
    "    main_curate_progress_message = \"Moving all files requested by the user\"\n",
    "    recursive_check_moved_files(dataset_structure)\n",
    "    main_curate_progress_message = \"Moved all files requested by the user\"\n",
    "\n",
    "    # 4. Rename any blackfynn files that are marked as renamed. \n",
    "    main_curate_progress_message = \"Renaming all files requested by the user\"\n",
    "    recursive_file_rename(dataset_structure)\n",
    "    main_curate_progress_message = \"Renamed all files requested by the user\"\n",
    "\n",
    "    # 5. Delete any blackfynn folders that are marked as deleted. \n",
    "    main_curate_progress_message = \"Deleting any additional folders present on blackfynn\"\n",
    "    recursive_folder_delete(dataset_structure)\n",
    "    main_curate_progress_message = \"Deletion of additional folders complete\"\n",
    "\n",
    "\n",
    "    # 6. Run the original code to upload any new files added to the dataset.\n",
    "    soda_json_structure[\"manifest-files\"] = {\"destination\": \"bf\"}\n",
    "    soda_json_structure[\"generate-dataset\"] = {\"destination\" : \"bf\", \"if-existing\": \"merge\", \"if-existing-files\": \"replace\"}\n",
    "    bf_generate_new_dataset(soda_json_structure, bf, ds)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_get_existing_folders_details(bf_folder):\n",
    "    bf_existing_folders = [x for x in bf_folder.items if x.type == \"Collection\"]\n",
    "    bf_existing_folders_name = [x.name for x in bf_existing_folders]\n",
    "\n",
    "    return bf_existing_folders, bf_existing_folders_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relative_path(x,y):\n",
    "    if x:\n",
    "        relative_path = x + '/' + y\n",
    "    else:\n",
    "        relative_path = y\n",
    "    return relative_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_get_existing_files_details(bf_folder):\n",
    "    bf_existing_files = [x for x in bf_folder.items if x.type != \"Collection\"]\n",
    "    bf_existing_files_name = [splitext(x.name)[0] for x in bf_existing_files]\n",
    "    bf_existing_files_name_with_extension = []\n",
    "    for file in bf_existing_files:\n",
    "        file_id = file.id\n",
    "        file_details = bf._api._get('/packages/' + str(file_id) + '/view')\n",
    "        file_name_with_extension = file_details[0][\"content\"][\"name\"]\n",
    "        file_extension = splitext(file_name_with_extension)[1]\n",
    "        file_name_with_extension = splitext(file.name)[0] + file_extension\n",
    "        bf_existing_files_name_with_extension.append(file_name_with_extension)\n",
    "\n",
    "    return bf_existing_files, bf_existing_files_name, bf_existing_files_name_with_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_file_name(file_name):\n",
    "    output = []\n",
    "    if file_name[-1] == ')':\n",
    "        string_length = len(file_name)\n",
    "        count_start = string_length\n",
    "        character = file_name[count_start-1]\n",
    "        while character != '(' and count_start>=0:\n",
    "            count_start -= 1\n",
    "            character = file_name[count_start-1]\n",
    "        if character == '(':\n",
    "            base_name = file_name[0:count_start-1]\n",
    "            num = file_name[count_start:string_length-1]\n",
    "            if check_if_int(num):\n",
    "                output = [base_name, int(num)]\n",
    "            return output\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_generate_new_dataset(soda_json_structure, bf, ds):\n",
    "\n",
    "    global main_curate_progress_message\n",
    "    global main_total_generate_dataset_size\n",
    "    global start_generate\n",
    "    global main_initial_bfdataset_size\n",
    "\n",
    "    try:\n",
    "\n",
    "        def recursive_create_folder_for_bf(my_folder, my_tracking_folder, existing_folder_option):\n",
    "\n",
    "            # list of existing bf folders at this level\n",
    "            my_bf_folder = my_tracking_folder[\"value\"]\n",
    "            my_bf_existing_folders, my_bf_existing_folders_name = bf_get_existing_folders_details(my_bf_folder)\n",
    "\n",
    "            # create/replace/skip folder\n",
    "            if \"folders\" in my_folder.keys():\n",
    "                my_tracking_folder[\"folders\"] = {}\n",
    "                for folder_key, folder in my_folder[\"folders\"].items():\n",
    "\n",
    "                    if existing_folder_option == \"skip\":\n",
    "                        if folder_key in my_bf_existing_folders_name:\n",
    "                            continue\n",
    "                        else:\n",
    "                            bf_folder = my_bf_folder.create_collection(folder_key)\n",
    "\n",
    "                    elif existing_folder_option == \"create-duplicate\":\n",
    "                        bf_folder = my_bf_folder.create_collection(folder_key)\n",
    "\n",
    "                    elif existing_folder_option == \"replace\":\n",
    "                        if folder_key in my_bf_existing_folders_name:\n",
    "                            index_folder = my_bf_existing_folders_name.index(folder_key)\n",
    "                            bf_folder_delete = my_bf_existing_folders[index_folder]\n",
    "                            bf_folder_delete.delete()\n",
    "                            my_bf_folder.update()\n",
    "                        bf_folder = my_bf_folder.create_collection(folder_key)\n",
    "\n",
    "                    elif existing_folder_option == \"merge\":\n",
    "                        if folder_key in my_bf_existing_folders_name:\n",
    "                            index_folder = my_bf_existing_folders_name.index(folder_key)\n",
    "                            bf_folder = my_bf_existing_folders[index_folder]\n",
    "                        else:\n",
    "                            bf_folder = my_bf_folder.create_collection(folder_key)\n",
    "                    bf_folder.update()\n",
    "                    my_tracking_folder[\"folders\"][folder_key] = {\"value\": bf_folder}\n",
    "                    tracking_folder = my_tracking_folder[\"folders\"][folder_key]\n",
    "                    recursive_create_folder_for_bf(folder, tracking_folder, existing_folder_option)\n",
    "\n",
    "        def recursive_dataset_scan_for_bf(my_folder, my_tracking_folder, existing_file_option, list_upload_files, my_relative_path):\n",
    "\n",
    "            global main_total_generate_dataset_size\n",
    "\n",
    "            my_bf_folder = my_tracking_folder[\"value\"]\n",
    "\n",
    "            if \"folders\" in my_folder.keys():\n",
    "                my_bf_existing_folders, my_bf_existing_folders_name = bf_get_existing_folders_details(my_bf_folder)\n",
    "\n",
    "                for folder_key, folder in my_folder[\"folders\"].items():\n",
    "                    relative_path = generate_relative_path(my_relative_path, folder_key)\n",
    "\n",
    "                    if existing_folder_option == \"skip\":\n",
    "                        if folder_key in my_bf_existing_folders_name:\n",
    "                            continue\n",
    "\n",
    "                    tracking_folder = my_tracking_folder[\"folders\"][folder_key]\n",
    "                    list_upload_files = recursive_dataset_scan_for_bf(folder, tracking_folder, existing_file_option, list_upload_files, relative_path)\n",
    "\n",
    "            if \"files\" in my_folder.keys():\n",
    "\n",
    "                #delete files to be deleted\n",
    "                my_bf_existing_files, my_bf_existing_files_name, my_bf_existing_files_name_with_extension = bf_get_existing_files_details(my_bf_folder)\n",
    "                for file_key, file in my_folder[\"files\"].items():\n",
    "                    if file[\"type\"] == \"local\":\n",
    "                        file_path = file[\"path\"]\n",
    "                        if isfile(file_path):\n",
    "                            if existing_file_option == \"replace\":\n",
    "                                if file_key in my_bf_existing_files_name_with_extension:\n",
    "                                    index_file = my_bf_existing_files_name_with_extension.index(file_key)\n",
    "                                    my_file = my_bf_existing_files[index_file]\n",
    "                                    my_file.delete()\n",
    "                                    my_bf_folder.update()\n",
    "\n",
    "                #create list of files to be uploaded with projected and desired names saved\n",
    "                my_bf_existing_files, my_bf_existing_files_name, my_bf_existing_files_name_with_extension = bf_get_existing_files_details(my_bf_folder)\n",
    "\n",
    "                list_local_files = []\n",
    "                list_projected_names = []\n",
    "                list_desired_names = []\n",
    "                list_final_names = []\n",
    "                additional_upload_lists = []\n",
    "                additional_list_count = 0\n",
    "                list_upload_schedule_projected_names = []\n",
    "                list_initial_names = []\n",
    "                for file_key, file in my_folder[\"files\"].items():\n",
    "                    if file[\"type\"] == \"local\":\n",
    "                        file_path = file[\"path\"]\n",
    "                        if isfile(file_path):\n",
    "\n",
    "                            initial_name = splitext(basename(file_path))[0]\n",
    "                            initial_extension = splitext(basename(file_path))[1]\n",
    "                            initial_name_with_extention = basename(file_path)\n",
    "                            desired_name = splitext(file_key)[0]\n",
    "\n",
    "                            if existing_file_option == \"skip\":\n",
    "                                if file_key in my_bf_existing_files_name_with_extension:\n",
    "                                    continue\n",
    "\n",
    "                            # check if initial filename exists on Blackfynn dataset and get the projected name of the file after upload\n",
    "                            count_done = 0\n",
    "                            count_exist = 0\n",
    "                            if initial_extension in bf_recognized_file_extensions:\n",
    "                                projected_name = initial_name\n",
    "                                while count_done == 0:\n",
    "                                    if projected_name in my_bf_existing_files_name:\n",
    "                                        count_exist += 1\n",
    "                                        projected_name = initial_name + \" (\" + str(count_exist) + \")\"\n",
    "                                    else:\n",
    "                                        count_done = 1\n",
    "                            else:\n",
    "                                count_done = 0\n",
    "                                count_exist = 0\n",
    "                                projected_name = initial_name_with_extention\n",
    "                                while count_done == 0:\n",
    "                                    if projected_name in my_bf_existing_files_name_with_extension:\n",
    "                                        count_exist += 1\n",
    "                                        projected_name = initial_name + \" (\" + str(count_exist) + \")\" + initial_extension\n",
    "                                    else:\n",
    "                                        count_done = 1\n",
    "\n",
    "                            # expected final name\n",
    "                            count_done = 0\n",
    "                            final_name = desired_name\n",
    "                            output = get_base_file_name(desired_name)\n",
    "                            if output:\n",
    "                                base_name = output[0]\n",
    "                                count_exist = output[1]\n",
    "                                while count_done == 0:\n",
    "                                    if final_name in my_bf_existing_files_name:\n",
    "                                        count_exist += 1\n",
    "                                        final_name = base_name + \"(\" + str(count_exist) + \")\"\n",
    "                                    else:\n",
    "                                        count_done = 1\n",
    "                            else:\n",
    "                                count_exist = 0\n",
    "                                while count_done == 0:\n",
    "                                    if final_name in my_bf_existing_files_name:\n",
    "                                        count_exist += 1\n",
    "                                        final_name = desired_name + \" (\" + str(count_exist) + \")\"\n",
    "                                    else:\n",
    "                                        count_done = 1\n",
    "\n",
    "                            # save in list accordingly\n",
    "                            if initial_name in list_initial_names or initial_name in list_final_names or projected_name in list_final_names or final_name in list_projected_names:\n",
    "                                additional_upload_lists.append([[file_path], my_bf_folder, [projected_name], [desired_name], [final_name], my_tracking_folder, my_relative_path])\n",
    "                            else:\n",
    "                                list_local_files.append(file_path)\n",
    "                                list_projected_names.append(projected_name)\n",
    "                                list_desired_names.append(desired_name)\n",
    "                                list_final_names.append(final_name)\n",
    "                                list_initial_names.append(initial_name)\n",
    "\n",
    "                            my_bf_existing_files_name.append(final_name)\n",
    "                            if initial_extension in bf_recognized_file_extensions:\n",
    "                                my_bf_existing_files_name_with_extension.append(final_name)\n",
    "                            else:\n",
    "                                my_bf_existing_files_name_with_extension.append(final_name + initial_extension)\n",
    "\n",
    "                            # add to projected dataset size to be generated\n",
    "                            main_total_generate_dataset_size += getsize(file_path)\n",
    "\n",
    "                if list_local_files:\n",
    "                    list_upload_files.append([list_local_files, my_bf_folder, list_projected_names, list_desired_names, list_final_names, my_tracking_folder, my_relative_path])\n",
    "\n",
    "                for item in additional_upload_lists:\n",
    "                    list_upload_files.append(item)\n",
    "\n",
    "            return list_upload_files\n",
    "\n",
    "\n",
    "        # 1. Scan the dataset structure to create all non-existent folders\n",
    "        # create a tracking dict which would track the generation of the dataset on Blackfynn\n",
    "        main_curate_progress_message = \"Creating folder structure\"\n",
    "        dataset_structure = soda_json_structure[\"dataset-structure\"]\n",
    "        tracking_json_structure = {\"value\": ds}\n",
    "        existing_folder_option = soda_json_structure[\"generate-dataset\"][\"if-existing\"]\n",
    "        recursive_create_folder_for_bf(dataset_structure, tracking_json_structure, existing_folder_option)\n",
    "\n",
    "        # 2. Scan the dataset structure and compile a list of files to be uploaded along with desired renaming\n",
    "        ds.update()\n",
    "        main_curate_progress_message = \"Preparing a list of files to upload\"\n",
    "        existing_file_option = soda_json_structure[\"generate-dataset\"][\"if-existing-files\"]\n",
    "        list_upload_files = []\n",
    "        relative_path = ds.name\n",
    "        list_upload_files = recursive_dataset_scan_for_bf(dataset_structure, tracking_json_structure, existing_file_option, list_upload_files, relative_path)\n",
    "        print(list_upload_files)\n",
    "\n",
    "        # 3. Add high-level metadata files to a list\n",
    "        ds.update()\n",
    "        list_upload_metadata_files = []\n",
    "        if \"metadata-files\" in soda_json_structure.keys():\n",
    "\n",
    "            my_bf_existing_files, my_bf_existing_files_name, my_bf_existing_files_name_with_extension = bf_get_existing_files_details(ds)\n",
    "            metadata_files = soda_json_structure[\"metadata-files\"]\n",
    "            for file_key, file in metadata_files.items():\n",
    "                if file[\"type\"] == \"local\":\n",
    "                    metadata_path = file[\"path\"]\n",
    "                    if isfile(metadata_path):\n",
    "                        initial_name = splitext(basename(metadata_path))[0]\n",
    "                        if existing_file_option == \"replace\":\n",
    "                            if initial_name in my_bf_existing_files_name:\n",
    "                                index_file = my_bf_existing_files_name.index(initial_name)\n",
    "                                my_file = my_bf_existing_files[index_file]\n",
    "                                my_file.delete()\n",
    "                                \n",
    "                        if existing_file_option == \"skip\":\n",
    "                            if initial_name in my_bf_existing_files_name:\n",
    "                                continue\n",
    "\n",
    "                        list_upload_metadata_files.append(metadata_path)\n",
    "                        main_total_generate_dataset_size += getsize(metadata_path)\n",
    "            \n",
    "        # 4. Prepare and add manifest files to a list\n",
    "        list_upload_manifest_files = []\n",
    "        if \"manifest-files\" in soda_json_structure.keys():\n",
    "\n",
    "            # prepare manifest files\n",
    "            if soda_json_structure[\"generate-dataset\"][\"destination\"] == \"bf\" and \"dataset-name\" not in soda_json_structure[\"generate-dataset\"]:\n",
    "                #generating dataset on an existing bf dataset - account for existing files and manifest files\n",
    "                manifest_files_structure = create_high_level_manifest_files_existing_bf(soda_json_structure, bf, ds)\n",
    "            else:\n",
    "                #generating on new bf\n",
    "                manifest_files_structure = create_high_level_manifest_files(soda_json_structure)\n",
    "\n",
    "            # add manifest files to list after deleting existing ones\n",
    "            list_upload_manifest_files = []\n",
    "            for key in manifest_files_structure.keys():\n",
    "                manifestpath = manifest_files_structure[key]\n",
    "                item = tracking_json_structure['folders'][key]['value']\n",
    "                destination_folder_id = item.id\n",
    "                #delete existing manifest files\n",
    "                for subitem in item:\n",
    "                    if subitem.name == \"manifest\":\n",
    "                        subitem.delete()\n",
    "                        item.update()\n",
    "                #upload new manifest files\n",
    "                list_upload_manifest_files.append([[manifestpath], item])\n",
    "                main_total_generate_dataset_size += getsize(manifestpath)\n",
    "\n",
    "        # 5. Upload files, rename, and add to tracking list\n",
    "        '''\n",
    "        main_initial_bfdataset_size = bf_dataset_size()\n",
    "        start_generate = 1\n",
    "        for item in list_upload_files:\n",
    "            list_upload = item[0]\n",
    "            bf_folder = item[1]\n",
    "            list_projected_names = item[2]\n",
    "            list_desired_names = item[3]\n",
    "            list_final_names = item[4]\n",
    "            tracking_folder = item[5]\n",
    "            relative_path = item[6]\n",
    "\n",
    "            #upload\n",
    "            main_curate_progress_message = \"Uploading files in \" + str(relative_path)\n",
    "            bf_folder.upload(*list_upload)\n",
    "            #bf_folder.update()\n",
    "\n",
    "            #rename to final name\n",
    "            for index, projected_name in enumerate(list_projected_names):\n",
    "                final_name = list_final_names[index]\n",
    "                desired_name = list_desired_names[index]\n",
    "                if final_name != projected_name:\n",
    "                    bf_item_list = bf_folder.items\n",
    "                    my_bf_existing_files, my_bf_existing_files_name, my_bf_existing_files_name_with_extension = bf_get_existing_files_details(bf_folder)\n",
    "                    for item in my_bf_existing_files:\n",
    "                        if item.name == projected_name:\n",
    "                            item.name = final_name\n",
    "                            item.update()\n",
    "                            if \"files\" not in tracking_folder:\n",
    "                                tracking_folder[\"files\"] = {}\n",
    "                            tracking_folder[\"files\"][desired_name] = {\"value\": item}\n",
    "\n",
    "        if list_upload_metadata_files:\n",
    "            main_curate_progress_message = \"Uploading metadata files in high-level dataset folder \" + str(ds.name)\n",
    "            ds.upload(*list_upload_metadata_files)\n",
    "\n",
    "        if list_upload_manifest_files:\n",
    "            for item in list_upload_manifest_files:\n",
    "                manifest_file = item[0]\n",
    "                bf_folder = item[1]\n",
    "                main_curate_progress_message = \"Uploading manifest file in \" + str(bf_folder.name) + \" folder\"\n",
    "                bf_folder.upload(*manifest_file)\n",
    "                #bf_folder.update()\n",
    "        shutil.rmtree(manifest_folder_path) if isdir(manifest_folder_path) else 0\n",
    "        '''\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = {\n",
    "    \"bf-account-selected\":{\"account-name\":\"calmilinux\"},\n",
    "    \"bf-dataset-selected\":{\"dataset-name\":\"testddataset\"},\n",
    "    \"dataset-structure\":{\n",
    "        \"folders\":{\n",
    "            \"code\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:collection:cdeabbcd-47b6-4f0b-b500-56f5e67fc0b6\",\n",
    "                    \"folders\":{\n",
    "                        \"fhj\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:collection:67f32258-459e-4db6-a884-859ab38b2efb\",\n",
    "                               \"folders\":{\n",
    "                                   \"png\":{\"type\":\"local\",\"path\":\"/home/dev/Desktop/SODA/src/assets/app-icon/png\",\n",
    "                                          \"folders\":{},\n",
    "                                          \"files\":{\n",
    "                                              \"icon.ico\":{\"path\":\"/home/dev/Desktop/SODA/src/assets/app-icon/png/icon.ico\",\"description\":\"\",\"additional-metadata\":\"\",\"type\":\"local\",\"action\":[\"new\"]},\n",
    "                                              \"soda_icon.png\":{\"path\":\"/home/dev/Desktop/SODA/src/assets/app-icon/png/soda_icon.png\",\"description\":\"\",\"additional-metadata\":\"\",\"type\":\"local\",\"action\":[\"new\"]}},\n",
    "                                          \"action\":[\"new\"]}},\n",
    "                               \"files\":{}},},\n",
    "                    \"files\":{\n",
    "                        \"test.odb\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:package:7e2651f9-3124-4ade-aff1-9d36bf3c45e2\",\"timestamp\":\"2020-12-21T23:28:11.934797Z\"},\n",
    "                        \"soda_icon2.png\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:package:ce7e3c3d-7111-4c4c-8a7c-ef1554dcaa9f\",\"timestamp\":\"2020-12-21T18:41:52.399846Z\"},\n",
    "                        \"soda_icon.png\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:package:5294f17d-0de1-4023-9322-4eae7c60eff0\",\"timestamp\":\"2020-12-21T18:41:51.311691Z\"}}},\n",
    "            \"primary\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:collection:95155ae6-df69-45f4-8493-0b988720c92a\",\n",
    "                       \"folders\":{\n",
    "                           \"png\":{\"type\":\"local\",\"path\":\"/home/dev/Desktop/SODA/src/assets/app-icon/png\",\n",
    "                                  \"folders\":{},\n",
    "                                  \"files\":{\n",
    "                                      \"icon.ico\":{\"path\":\"/home/dev/Desktop/SODA/src/assets/app-icon/png/icon.ico\",\"description\":\"\",\"additional-metadata\":\"\",\"type\":\"local\",\"action\":[\"new\"]},\n",
    "                                      \"soda_icon.png\":{\"path\":\"/home/dev/Desktop/SODA/src/assets/app-icon/png/soda_icon.png\",\"description\":\"\",\"additional-metadata\":\"\",\"type\":\"local\",\"action\":[\"new\"]}},\n",
    "                                  \"action\":[\"new\"]}},\n",
    "                       \"files\":{}},\n",
    "            \"source\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:collection:2994d497-37c0-41c9-b88a-1daff9a7c0bc\",\n",
    "                      \"folders\":{\n",
    "                          \"png\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:collection:252325e6-ee96-4a79-9379-1b3536d33877\",\n",
    "                                 \"folders\":{},\n",
    "                                 \"files\":{\n",
    "                                     \"soda_icon (1).png\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:package:ed1ae9a2-5067-428a-9dde-65a76b5f2732\",\"timestamp\":\"2020-12-21T19:16:17.28051Z\"},\n",
    "                                     \"soda_icon2.png\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:package:ab3029d9-31f0-49d5-9e37-97cd1066692a\",\"timestamp\":\"2020-12-21T19:16:18.86915Z\"},\n",
    "                                     \"icon.ico\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:package:1d277d08-b129-4684-b611-1d33ddcdda2a\",\"timestamp\":\"2020-12-21T19:16:18.124946Z\"}}}},\n",
    "                      \"files\":{}}}},\n",
    "    \"metadata-files\":{\n",
    "        \"submission.csv\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:package:3b504e70-58ca-4bfb-8c80-f214f32ccaa0\"},\n",
    "        \"subjects.csv\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:package:41295a11-4fe0-4d4d-95fb-26b50646f74b\"},\n",
    "        \"dataset_description.csv\":{\"type\":\"bf\",\"action\":[\"existing\"],\"path\":\"N:package:69a01be0-42c3-4129-9898-4aa47734b4ea\"}},\n",
    "    \"manifest-files\":{},\n",
    "    \"generate-dataset\":{},\n",
    "    \"starting-point\":\"bf\"}\n",
    "sdmy = sd[\"dataset-structure\"]\n",
    "sdbf = bf_get_dataset_files_folders(sd.copy())[0][\"dataset-structure\"]\n",
    "mysd = sdmy.copy()\n",
    "bfsd = sdbf.copy()\n",
    "#path to local SODA folder for saving manifest files\n",
    "manifest_sparc = [\"manifest.xlsx\", \"manifest.csv\"]\n",
    "manifest_folder_path = join(userpath, 'SODA', 'manifest_files')\n",
    "main_total_generate_dataset_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd[\"manifest-files\"] = {\"destination\": \"bf\"}\n",
    "sd[\"generate-dataset\"] = {\"destination\" : \"bf\", \"if-existing\": \"merge\", \"if-existing-files\": \"create-duplicate\"}\n",
    "bf = Blackfynn(\"calmilinux\")\n",
    "myds = bf.get_dataset(\"testddataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '/home/dev/Desktop/SODA/src/assets/app-icon/png/icon.ico', 'description': '', 'additional-metadata': '', 'type': 'local', 'action': ['new']}\n",
      "{'path': '/home/dev/Desktop/SODA/src/assets/app-icon/png/soda_icon.png', 'description': '', 'additional-metadata': '', 'type': 'local', 'action': ['new']}\n",
      "{'path': '/home/dev/Desktop/SODA/src/assets/app-icon/png/icon.ico', 'description': '', 'additional-metadata': '', 'type': 'local', 'action': ['new']}\n",
      "{'path': '/home/dev/Desktop/SODA/src/assets/app-icon/png/soda_icon.png', 'description': '', 'additional-metadata': '', 'type': 'local', 'action': ['new']}\n",
      "[[['/home/dev/Desktop/SODA/src/assets/app-icon/png/icon.ico', '/home/dev/Desktop/SODA/src/assets/app-icon/png/soda_icon.png'], <Collection name='png' id='N:collection:3281ef66-20b0-4122-a768-d1eb45fa7b38'>, ['icon.ico', 'soda_icon'], ['icon', 'soda_icon'], ['icon', 'soda_icon'], {'value': <Collection name='png' id='N:collection:3281ef66-20b0-4122-a768-d1eb45fa7b38'>, 'folders': {}}, 'testddataset/code/fhj/png'], [['/home/dev/Desktop/SODA/src/assets/app-icon/png/icon.ico', '/home/dev/Desktop/SODA/src/assets/app-icon/png/soda_icon.png'], <Collection name='png' id='N:collection:a0723445-99b3-42f3-a5f7-a3b21054ddf8'>, ['icon.ico', 'soda_icon'], ['icon', 'soda_icon'], ['icon', 'soda_icon'], {'value': <Collection name='png' id='N:collection:a0723445-99b3-42f3-a5f7-a3b21054ddf8'>, 'folders': {}}, 'testddataset/primary/png']]\n"
     ]
    }
   ],
   "source": [
    "bf_generate_new_dataset(sd, bf, myds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
